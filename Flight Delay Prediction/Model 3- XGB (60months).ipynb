{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc4b6aae-1737-4047-8aa3-a7d3a56996a7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Loading in Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f35f3799-f835-4904-ac3f-635f9811dc4b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Standard libraries/functions\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Pyspark libraries/functions\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import col, sum, isnan, when, count, year, month, dayofmonth, date_format, concat_ws, acos, cos, radians, sin, udf, concat\n",
    "from pyspark.sql.types import IntegerType, DateType, DoubleType, StringType, FloatType\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# from pyspark.ml.linalg import Vectors#, VectorUDT\n",
    "# from pyspark.ml.linalg import VectorType\n",
    "from pyspark.mllib.linalg import Vectors, VectorUDT\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler, StringIndexer, OneHotEncoder, Imputer\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import PCA\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics, MulticlassMetrics\n",
    "\n",
    "# Sklearn libraries/functions\n",
    "from sklearn.utils import shuffle\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from pyspark.ml.classification import LogisticRegression, GBTClassifier, RandomForestClassifier, LinearSVC\n",
    "import time\n",
    "\n",
    "import mlflow\n",
    "import mlflow.spark\n",
    "# from pyspark.mllib.linalg import Vectors, VectorUDT\n",
    "from xgboost.spark import SparkXGBClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f7ca007-066a-42e6-acdd-58b8b67f35fe",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Setting Up Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84be5c3b-a766-4381-ab12-1e72d2c513de",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>wasbs://smsj-261@smsj.blob.core.windows.net/test/</td><td>test/</td><td>0</td><td>1689534418000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "wasbs://smsj-261@smsj.blob.core.windows.net/test/",
         "test/",
         0,
         1689534418000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "size",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "modificationTime",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Place this cell in any team notebook that needs access to the team cloud storage.\n",
    "\n",
    "# The following blob storage is accessible to team members only (read and write)\n",
    "# access key is valid til TTL\n",
    "# after that you will need to create a new SAS key and authenticate access again via DataBrick command line\n",
    "blob_container  = \"smsj-261\"       # The name of your container created in https://portal.azure.com\n",
    "storage_account = \"smsj\"  # The name of your Storage account created in https://portal.azure.com\n",
    "secret_scope    = \"smsjscope\"           # The name of the scope created in your local computer using the Databricks CLI\n",
    "secret_key      = \"smsjkey\"             # The name of the secret key created in your local computer using the Databricks CLI\n",
    "team_blob_url        = f\"wasbs://{blob_container}@{storage_account}.blob.core.windows.net\"  #points to the root of your team storage bucket\n",
    "\n",
    "# the 261 course blob storage is mounted here on the DataBricks workspace.\n",
    "mids261_mount_path      = \"/mnt/mids-w261\"\n",
    "\n",
    "# SAS Token: Grant the team limited access to Azure Storage resources\n",
    "spark.conf.set(\n",
    "  f\"fs.azure.sas.{blob_container}.{storage_account}.blob.core.windows.net\",\n",
    "  dbutils.secrets.get(scope = secret_scope, key = secret_key)\n",
    ")\n",
    "import pandas as pd\n",
    "pdf = pd.DataFrame([[1, 2, 3, \"Jane\"], [2, 2,2, None], [12, 12,12, \"John\"]], columns=[\"x\", \"y\", \"z\", \"a_string\"])\n",
    "df = spark.createDataFrame(pdf) # Create a Spark dataframe from a pandas DF\n",
    "\n",
    "# The following can write the dataframe to the team's Cloud Storage  \n",
    "# Navigate back to your Storage account in https://portal.azure.com, to inspect the partitions/files.\n",
    "# df.write.parquet(f\"{team_blob_url}/test\")\n",
    "\n",
    "# see what's in the blob storage root folder \n",
    "display(dbutils.fs.ls(f\"{team_blob_url}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7055ebcf-6d00-4fee-9b39-434aef06a0b6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41295f59-f792-4fcd-852a-fb98e216799a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# import spark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"ReadingParquet\").getOrCreate()\n",
    "\n",
    "df_test_3m = spark.read.parquet(\"dbfs:/user/hive/warehouse/test_60m\")\n",
    "df_val = spark.sql(\"SELECT * FROM train_60m where YEAR(FL_DATE) = 2018\")\n",
    "df_train_3m = spark.read.parquet(\"dbfs:/user/hive/warehouse/train_60m\")\n",
    "\n",
    "df_test_3m.createOrReplaceTempView(\"test_60m\")\n",
    "df_train_3m.createOrReplaceTempView(\"df_train_60m\")\n",
    "# Use the DataFrame in your ML Flow project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94140fe9-be7b-4bf4-9d8a-b30a0434955a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "9192041"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_train_3m.count()\n",
    "# df_val.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c80ce5cb-c08b-4ef4-8c42-ab5dfb4524d6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Modelling Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b6ab9b1-0f40-4519-ac68-8bdda07f5ab6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def extract(row):\n",
    "    return tuple(row.probability.toArray().tolist()) +  (row.label,) + (row.prediction,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "072487ac-6711-4c16-8755-994d4678981a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def RegressionEvaluator(preds):\n",
    "\n",
    "    rdd_preds_m = preds.select(['prediction', 'label']).rdd\n",
    "\n",
    "    preds = preds.select(\"probability\", \"label\", \"prediction\")\n",
    "    preds = preds.rdd.map(extract).toDF([\"p0\", \"p1\", \"label\", \"prediction\"])\n",
    "\n",
    "    # Create an binary evaluator\n",
    "    evaluator_auc = BinaryClassificationEvaluator(labelCol='label')\n",
    "    evaluator_auc.setRawPredictionCol('p1')\n",
    "\n",
    "    # Compute the areaUnderROC on the test data\n",
    "    areaUnderROC = evaluator_auc.evaluate(preds, {evaluator_auc.metricName: \"areaUnderROC\"})\n",
    "    areaUnderPR = evaluator_auc.evaluate(preds, {evaluator_auc.metricName: \"areaUnderPR\"})\n",
    "\n",
    "    multi_evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\")\n",
    "    multi_evaluator2 = MulticlassMetrics(rdd_preds_m)\n",
    "    # Compute various evaluation metrics\n",
    "    accuracy = multi_evaluator.evaluate(preds, {multi_evaluator.metricName: \"accuracy\"})\n",
    "    precision = multi_evaluator.evaluate(preds, {multi_evaluator.metricName: \"precisionByLabel\"})\n",
    "    recall = multi_evaluator.evaluate(preds, {multi_evaluator.metricName: \"recallByLabel\"})\n",
    "    f1 = multi_evaluator.evaluate(preds, {multi_evaluator.metricName: \"f1\"})\n",
    "\n",
    "    f2 = np.round(multi_evaluator2.fMeasure(label=1.0, beta=2.0), 5)\n",
    "    # pr = binary_evaluator.areaUnderPR\n",
    "\n",
    "    return accuracy, precision, recall, f1, f2 ,areaUnderROC,areaUnderPR\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ea78ecb-2cde-4653-bda6-eddd2eb7776b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Model Pipeline (Modelling & Hyperparameter Tuning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56eb562d-d09f-49e9-b772-909b45589a8e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# ML Flow Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3dcf313d-4155-44a9-b242-f542db57251b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'num_workers': <hyperopt.pyll.base.Apply at 0x7f1840b49990>,\n",
       " 'n_estimators': <hyperopt.pyll.base.Apply at 0x7f1840bbf550>,\n",
       " 'max_bin': <hyperopt.pyll.base.Apply at 0x7f1840bbdf30>,\n",
       " 'max_depth': <hyperopt.pyll.base.Apply at 0x7f1840bc9450>,\n",
       " 'learning_rate': <hyperopt.pyll.base.Apply at 0x7f1840bc9300>,\n",
       " 'max_leaves': <hyperopt.pyll.base.Apply at 0x7f1840bc91b0>,\n",
       " 'gamma': <hyperopt.pyll.base.Apply at 0x7f1840bca6b0>,\n",
       " 'scale_pos_weight': <hyperopt.pyll.base.Apply at 0x7f1840bca350>,\n",
       " 'colsample_bytree': <hyperopt.pyll.base.Apply at 0x7f1840bca200>}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb = SparkXGBClassifier(features_col=\"allFeatures\")\n",
    "df_train = df_train_3m\n",
    "df_test = df_test_3m\n",
    "\n",
    "pipeline = Pipeline(stages=[xgb])\n",
    "\n",
    "from hyperopt import hp\n",
    "search_space = {\n",
    "    \"num_workers\": hp.quniform(\"num_workers\", 3,5,1),\n",
    "    \"n_estimators\": hp.quniform(\"n_estimators\", 90,120.0,10.0),\n",
    "    \"max_bin\": hp.quniform(\"max_bin\", 25.0,35.0,3.0),\n",
    "    \"max_depth\": hp.quniform(\"max_depth\", 6,16,2),\n",
    "    \"learning_rate\": hp.quniform(\"learning_rate\", 0.2,0.6,0.1),\n",
    "    \"max_leaves\": hp.quniform(\"max_leaves\", 6,16,1),\n",
    "    \"gamma\": hp.quniform(\"gamma\", 0,20,2),\n",
    "    \"scale_pos_weight\": hp.quniform(\"scale_pos_weight\", 0.8,2,0.2),\n",
    "    \"colsample_bytree\": hp.quniform(\"colsample_bytree\", 0.75,1,0.05),\n",
    "}\n",
    "search_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f3b692f-fe72-4e8d-84e8-a933dbcaf8d7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def objective_function(params):\n",
    "    # CHANGES HERE\n",
    "    num_workers = params[\"num_workers\"]\n",
    "    n_estimators = params[\"n_estimators\"]\n",
    "    max_bin = params[\"max_bin\"]\n",
    "    max_depth = params[\"max_depth\"]\n",
    "    learning_rate = params[\"learning_rate\"]\n",
    "    max_leaves = params[\"max_leaves\"]\n",
    "    gamma = params[\"gamma\"]\n",
    "    scale_pos_weight = params[\"scale_pos_weight\"]\n",
    "    # reg_alpha = params[\"reg_alpha\"]\n",
    "    # min_child_weight = params[\"min_child_weight\"]\n",
    "\n",
    "\n",
    "    with mlflow.start_run():\n",
    "        # CHANGES HERE\n",
    "        estimator = pipeline.copy({ xgb.num_workers:num_workers,\n",
    "                                    xgb.n_estimators:int(n_estimators),\n",
    "                                    xgb.max_bin:int(max_bin),\n",
    "                                    xgb.max_depth: int(max_depth),\n",
    "                                    xgb.learning_rate:learning_rate,\n",
    "                                    xgb.max_leaves:int(max_leaves),\n",
    "                                    xgb.gamma:gamma,\n",
    "                                    xgb.scale_pos_weight:scale_pos_weight\n",
    "                                    # ,\n",
    "                                    # xgb.reg_alpha:reg_alpha,\n",
    "                                    # xgb.min_child_weight:min_child_weight\n",
    "\n",
    "\n",
    "                                  })\n",
    "        \n",
    "\n",
    "        model = estimator.fit(df_train)\n",
    "\n",
    "        preds_training = model.transform(df_train)       \n",
    "        pred_calc_training = RegressionEvaluator(preds_training)  \n",
    "\n",
    "        train_accuracy = pred_calc_training[0]\n",
    "        train_precision = pred_calc_training[1]\n",
    "        train_recall = pred_calc_training[2]\n",
    "        train_f1_score = pred_calc_training[3]\n",
    "        train_f2_score = pred_calc_training[4]\n",
    "        train_areaUnderROC = pred_calc_training[5]\n",
    "        train_areaUnderPR = pred_calc_training[6]\n",
    "\n",
    "        mlflow.log_metric('train_accuracy', train_accuracy)\n",
    "        mlflow.log_metric('train_precision', train_precision)\n",
    "        mlflow.log_metric('train_recall', train_recall)\n",
    "        mlflow.log_metric('train_f1_score', train_f1_score)\n",
    "        mlflow.log_metric('train_f2_score', train_f2_score)\n",
    "        mlflow.log_metric('train_areaUnderROC', train_areaUnderROC)\n",
    "        mlflow.log_metric('train_areaUnderPR', train_areaUnderPR)\n",
    "\n",
    "\n",
    "        print('-------------------')\n",
    "        print('Train Metrics:')\n",
    "        print('accuracy:',train_accuracy)\n",
    "        print('precision:',train_precision)\n",
    "        print('recall:',train_recall)\n",
    "\n",
    "        print('f1_score:',train_f1_score)\n",
    "        print('f2_score:',train_f2_score)\n",
    "\n",
    "        print('areaUnderROC:',str(train_areaUnderROC))\n",
    "        print('areaUnderPR:',str(train_areaUnderPR))\n",
    "\n",
    "        \n",
    "        preds = model.transform(df_test)\n",
    "        # preds = model.transform(df_val)\n",
    "        pred_calc = RegressionEvaluator(preds)\n",
    "        val_accuracy = pred_calc[0]\n",
    "        val_precision = pred_calc[1]\n",
    "        val_recall = pred_calc[2]\n",
    "        val_f1_score = pred_calc[3]\n",
    "        val_f2_score = pred_calc[4]\n",
    "        val_areaUnderROC = pred_calc[5]\n",
    "        val_areaUnderPR = pred_calc[6]\n",
    "\n",
    "        mlflow.log_metric('val_accuracy', val_accuracy)\n",
    "        mlflow.log_metric('val_precision', val_precision)\n",
    "        mlflow.log_metric('val_recall', val_recall)\n",
    "        mlflow.log_metric('val_f1_score', val_f1_score)\n",
    "        mlflow.log_metric('val_f2_score', val_f2_score)\n",
    "        mlflow.log_metric('val_areaUnderROC', val_areaUnderROC)\n",
    "        mlflow.log_metric('val_areaUnderPR', val_areaUnderPR)\n",
    "        print('-------------------')\n",
    "        print('Validation Metrics:')\n",
    "        print('accuracy:',val_accuracy)\n",
    "        print('precision:',val_precision)\n",
    "        print('recall:',val_recall)\n",
    "        print('f1_score:',val_f1_score)\n",
    "        print('f2_score:',val_f2_score)\n",
    "        print('areaUnderROC:',val_areaUnderROC)\n",
    "        print('areaUnderPR:',val_areaUnderPR)\n",
    "\n",
    "\n",
    "        print('-------------------')\n",
    "        print('Model Params:')\n",
    "        print('num_workers:',num_workers)\n",
    "        print('n_estimators:',n_estimators)\n",
    "        print('max_bin:',max_bin)\n",
    "        print('max_depth:',max_depth)\n",
    "        print('learning_rate:',learning_rate)\n",
    "        print('max_leaves:',max_leaves)\n",
    "        print('gamma:',gamma)\n",
    "\n",
    "        mlflow.spark.log_model(model, \"XGB_Model_test_JT_60m\")\n",
    "        # print('model_logging_complete' + \" --- %s seconds ---\" % (time.time() - start_time))\n",
    "    return val_areaUnderPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0adb2886-d237-4c1c-b6d8-d1f898a416e5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r  0%|          | 0/1 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/08/08 21:30:21 WARNING mlflow.data.spark_dataset: Failed to infer schema for Spark dataset. Exception: Unsupported Spark Type '<class 'pyspark.ml.linalg.VectorUDT'>', MLflow schema is only supported for scalar Spark types.\n\n/databricks/python/lib/python3.10/site-packages/xgboost/sklearn.py:782: UserWarning: Loading a native XGBoost model with Scikit-Learn interface.\n  warnings.warn(\"Loading a native XGBoost model with Scikit-Learn interface.\")\n\n/databricks/spark/python/pyspark/sql/context.py:165: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n  warnings.warn(\n\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r                                                     \r-------------------\n\r  0%|          | 0/1 [13:16<?, ?trial/s, best loss=?]\r                                                     \rTrain Metrics:\n\r  0%|          | 0/1 [13:16<?, ?trial/s, best loss=?]\r                                                     \raccuracy:\n\r  0%|          | 0/1 [13:16<?, ?trial/s, best loss=?]\r                                                     \r0.8134354492108988\n\r  0%|          | 0/1 [13:16<?, ?trial/s, best loss=?]\r                                                     \rprecision:\n\r  0%|          | 0/1 [13:16<?, ?trial/s, best loss=?]\r                                                     \r0.8419836580548854\n\r  0%|          | 0/1 [13:16<?, ?trial/s, best loss=?]\r                                                     \rrecall:\n\r  0%|          | 0/1 [13:16<?, ?trial/s, best loss=?]\r                                                     \r0.951528868476467\n\r  0%|          | 0/1 [13:16<?, ?trial/s, best loss=?]\r                                                     \rf1_score:\n\r  0%|          | 0/1 [13:16<?, ?trial/s, best loss=?]\r                                                     \r0.7791927723013806\n\r  0%|          | 0/1 [13:16<?, ?trial/s, best loss=?]\r                                                     \rf2_score:\n\r  0%|          | 0/1 [13:16<?, ?trial/s, best loss=?]\r                                                     \r0.20113\n\r  0%|          | 0/1 [13:16<?, ?trial/s, best loss=?]\r                                                     \rareaUnderROC:\n\r  0%|          | 0/1 [13:16<?, ?trial/s, best loss=?]\r                                                     \r0.7044750745455559\n\r  0%|          | 0/1 [13:16<?, ?trial/s, best loss=?]\r                                                     \rareaUnderPR:\n\r  0%|          | 0/1 [13:16<?, ?trial/s, best loss=?]\r                                                     \r0.3430680078106414\n\r  0%|          | 0/1 [13:16<?, ?trial/s, best loss=?]\r                                                     \r-------------------\n\r  0%|          | 0/1 [14:58<?, ?trial/s, best loss=?]\r                                                     \rValidation Metrics:\n\r  0%|          | 0/1 [14:58<?, ?trial/s, best loss=?]\r                                                     \raccuracy:\n\r  0%|          | 0/1 [14:58<?, ?trial/s, best loss=?]\r                                                     \r0.8021479188973327\n\r  0%|          | 0/1 [14:58<?, ?trial/s, best loss=?]\r                                                     \rprecision:\n\r  0%|          | 0/1 [14:58<?, ?trial/s, best loss=?]\r                                                     \r0.8388340319348109\n\r  0%|          | 0/1 [14:58<?, ?trial/s, best loss=?]\r                                                     \rrecall:\n\r  0%|          | 0/1 [14:58<?, ?trial/s, best loss=?]\r                                                     \r0.9379310774440249\n\r  0%|          | 0/1 [14:58<?, ?trial/s, best loss=?]\r                                                     \rf1_score:\n\r  0%|          | 0/1 [14:58<?, ?trial/s, best loss=?]\r                                                     \r0.7723482871687344\n\r  0%|          | 0/1 [14:58<?, ?trial/s, best loss=?]\r                                                     \rf2_score:\n\r  0%|          | 0/1 [14:58<?, ?trial/s, best loss=?]\r                                                     \r0.2206\n\r  0%|          | 0/1 [14:58<?, ?trial/s, best loss=?]\r                                                     \rareaUnderROC:\n\r  0%|          | 0/1 [14:58<?, ?trial/s, best loss=?]\r                                                     \r0.692003172630747\n\r  0%|          | 0/1 [14:58<?, ?trial/s, best loss=?]\r                                                     \rareaUnderPR:\n\r  0%|          | 0/1 [14:58<?, ?trial/s, best loss=?]\r                                                     \r0.3363465582622245\n\r  0%|          | 0/1 [14:58<?, ?trial/s, best loss=?]\r                                                     \r-------------------\n\r  0%|          | 0/1 [14:58<?, ?trial/s, best loss=?]\r                                                     \rModel Params:\n\r  0%|          | 0/1 [14:58<?, ?trial/s, best loss=?]\r                                                     \rnum_workers:\n\r  0%|          | 0/1 [14:58<?, ?trial/s, best loss=?]\r                                                     \r5.0\n\r  0%|          | 0/1 [14:58<?, ?trial/s, best loss=?]\r                                                     \rn_estimators:\n\r  0%|          | 0/1 [14:58<?, ?trial/s, best loss=?]\r                                                     \r120.0\n\r  0%|          | 0/1 [14:58<?, ?trial/s, best loss=?]\r                                                     \rmax_bin:\n\r  0%|          | 0/1 [14:58<?, ?trial/s, best loss=?]\r                                                     \r30.0\n\r  0%|          | 0/1 [14:58<?, ?trial/s, best loss=?]\r                                                     \rmax_depth:\n\r  0%|          | 0/1 [14:58<?, ?trial/s, best loss=?]\r                                                     \r14.0\n\r  0%|          | 0/1 [14:58<?, ?trial/s, best loss=?]\r                                                     \rlearning_rate:\n\r  0%|          | 0/1 [14:58<?, ?trial/s, best loss=?]\r                                                     \r0.5\n\r  0%|          | 0/1 [14:58<?, ?trial/s, best loss=?]\r                                                     \rmax_leaves:\n\r  0%|          | 0/1 [14:58<?, ?trial/s, best loss=?]\r                                                     \r14.0\n\r  0%|          | 0/1 [14:58<?, ?trial/s, best loss=?]\r                                                     \rgamma:\n\r  0%|          | 0/1 [14:58<?, ?trial/s, best loss=?]\r                                                     \r6.0\n\r  0%|          | 0/1 [14:58<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/08/08 21:45:20 INFO mlflow.spark: Inferring pip requirements by reloading the logged model from the databricks artifact repository, which can be time-consuming. To speed up, explicitly specify the conda_env or pip_requirements when calling log_model().\n\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r100%|██████████| 1/1 [15:36<00:00, 936.13s/trial, best loss: 0.3363465582622245]\r100%|██████████| 1/1 [15:36<00:00, 936.13s/trial, best loss: 0.3363465582622245]\n"
     ]
    }
   ],
   "source": [
    "from hyperopt import fmin, tpe, Trials\n",
    "import mlflow\n",
    "\n",
    "mlflow.pyspark.ml.autolog(log_models=False)\n",
    "num_evals = 1\n",
    "trials = Trials()\n",
    "best_hyperparam = fmin(fn=objective_function,\n",
    "                       space = search_space,\n",
    "                       algo=tpe.suggest,\n",
    "                       max_evals = num_evals,\n",
    "                       trials=trials,\n",
    "                       rstate=np.random.default_rng(42))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5701abe-bbcc-46af-9d7c-ee16dd0384f4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'colsample_bytree': 0.6000000000000001,\n",
       " 'gamma': 8.0,\n",
       " 'learning_rate': 0.5,\n",
       " 'max_bin': 70.0,\n",
       " 'max_depth': 8.0,\n",
       " 'max_leaves': 14.0,\n",
       " 'n_estimators': 160.0,\n",
       " 'num_workers': 4.0}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_hyperparam"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [
    {
     "elements": [],
     "globalVars": {},
     "guid": "49229e6c-f75e-4375-a9ca-5e2982b05331",
     "layoutOption": {
      "grid": true,
      "stack": true
     },
     "nuid": "d404ab4d-79db-4b22-8327-4df01cb5dc02",
     "origId": 3984215836275772,
     "title": "Untitled",
     "version": "DashboardViewV1",
     "width": 1024
    },
    {
     "elements": [],
     "globalVars": {},
     "guid": "31385d0d-70d0-4322-8d51-49f29ffa5a99",
     "layoutOption": {
      "grid": true,
      "stack": true
     },
     "nuid": "e5336f6e-92ad-4e64-8bc1-efa53b3cc9cb",
     "origId": 3984215836275773,
     "title": "Summary Table",
     "version": "DashboardViewV1",
     "width": 1024
    }
   ],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "FP_Section2_Group4_XGB_60month",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
