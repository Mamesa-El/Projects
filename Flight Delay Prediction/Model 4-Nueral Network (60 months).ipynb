{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc4b6aae-1737-4047-8aa3-a7d3a56996a7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Loading in Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f35f3799-f835-4904-ac3f-635f9811dc4b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.databricks.v1+bamboolib_hint": "{\"pd.DataFrames\": [], \"version\": \"0.0.1\"}",
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Standard libraries/functions\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Pyspark libraries/functions\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import col, sum, isnan, when, count, year, month, dayofmonth, date_format, concat_ws, acos, cos, radians, sin, udf, concat\n",
    "from pyspark.sql.types import IntegerType, DateType, DoubleType, StringType, FloatType\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# from pyspark.ml.linalg import Vectors#, VectorUDT\n",
    "# from pyspark.ml.linalg import VectorType\n",
    "from pyspark.mllib.linalg import Vectors, VectorUDT\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler, StringIndexer, OneHotEncoder, Imputer\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import PCA\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics, MulticlassMetrics\n",
    "\n",
    "# Sklearn libraries/functions\n",
    "from sklearn.utils import shuffle\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from pyspark.ml.classification import LogisticRegression, GBTClassifier, RandomForestClassifier, LinearSVC\n",
    "import time\n",
    "\n",
    "import mlflow\n",
    "import mlflow.spark\n",
    "# from pyspark.mllib.linalg import Vectors, VectorUDT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6dc1c5f0-7401-44bd-b399-45721ce659a8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## library settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f7ca007-066a-42e6-acdd-58b8b67f35fe",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Setting Up Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84be5c3b-a766-4381-ab12-1e72d2c513de",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>wasbs://smsj-261@smsj.blob.core.windows.net/test/</td><td>test/</td><td>0</td><td>1689534418000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "wasbs://smsj-261@smsj.blob.core.windows.net/test/",
         "test/",
         0,
         1689534418000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "size",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "modificationTime",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Place this cell in any team notebook that needs access to the team cloud storage.\n",
    "\n",
    "# The following blob storage is accessible to team members only (read and write)\n",
    "# access key is valid til TTL\n",
    "# after that you will need to create a new SAS key and authenticate access again via DataBrick command line\n",
    "blob_container  = \"smsj-261\"       # The name of your container created in https://portal.azure.com\n",
    "storage_account = \"smsj\"  # The name of your Storage account created in https://portal.azure.com\n",
    "secret_scope    = \"smsjscope\"           # The name of the scope created in your local computer using the Databricks CLI\n",
    "secret_key      = \"smsjkey\"             # The name of the secret key created in your local computer using the Databricks CLI\n",
    "team_blob_url        = f\"wasbs://{blob_container}@{storage_account}.blob.core.windows.net\"  #points to the root of your team storage bucket\n",
    "\n",
    "# the 261 course blob storage is mounted here on the DataBricks workspace.\n",
    "mids261_mount_path      = \"/mnt/mids-w261\"\n",
    "\n",
    "# SAS Token: Grant the team limited access to Azure Storage resources\n",
    "spark.conf.set(\n",
    "  f\"fs.azure.sas.{blob_container}.{storage_account}.blob.core.windows.net\",\n",
    "  dbutils.secrets.get(scope = secret_scope, key = secret_key)\n",
    ")\n",
    "import pandas as pd\n",
    "pdf = pd.DataFrame([[1, 2, 3, \"Jane\"], [2, 2,2, None], [12, 12,12, \"John\"]], columns=[\"x\", \"y\", \"z\", \"a_string\"])\n",
    "df = spark.createDataFrame(pdf) # Create a Spark dataframe from a pandas DF\n",
    "\n",
    "# The following can write the dataframe to the team's Cloud Storage  \n",
    "# Navigate back to your Storage account in https://portal.azure.com, to inspect the partitions/files.\n",
    "# df.write.parquet(f\"{team_blob_url}/test\")\n",
    "\n",
    "# see what's in the blob storage root folder \n",
    "display(dbutils.fs.ls(f\"{team_blob_url}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7055ebcf-6d00-4fee-9b39-434aef06a0b6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41295f59-f792-4fcd-852a-fb98e216799a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# import spark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"ReadingParquet\").getOrCreate()\n",
    "\n",
    "# df = spark.read.parquet(\"/path/to/parquet/file.parquet\")\n",
    "# df.show()\n",
    "\n",
    "# spark = spark.sql.SparkSession.builder.appName(\"Call Parquet File\").getOrCreate()\n",
    "\n",
    "df_test_60m = spark.read.parquet(\"dbfs:/user/hive/warehouse/test_60m\")\n",
    "df_train_60m = spark.read.parquet(\"dbfs:/user/hive/warehouse/train_60m\")\n",
    "\n",
    "\n",
    "\n",
    "# df = spark.read.parquet(\"s3://my-bucket/test_3m.parquet\")\n",
    "\n",
    "df_test_60m.createOrReplaceTempView(\"test_60m\")\n",
    "df_train_60m.createOrReplaceTempView(\"df_train_60m\")\n",
    "# Use the DataFrame in your ML Flow project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4173200d-c3a3-4b75-bcc1-852faa9516ab",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_val = spark.sql(\"SELECT * FROM train_60m where YEAR(FL_DATE) = 2018\")\n",
    "df_train_60m = spark.sql(\"SELECT * FROM train_60m where YEAR(FL_DATE) < 2018\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62fd3578-41f2-42b1-aa9e-607b1f1571ea",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Initializing Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4fa9a5d2-637e-4dbd-80f8-afb501d67f74",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Reading the Data Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c80ce5cb-c08b-4ef4-8c42-ab5dfb4524d6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Modelling Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e666118f-5ad0-480b-89d3-1553a86dd56d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def build_model (model_name, param_dict):\n",
    "    '''\n",
    "    Description: Contains models and hyperparameters\n",
    "    Inputs:\n",
    "    Outputs: \n",
    "    '''\n",
    "    # Build Logistic Regression model\n",
    "    if model_name == 'log':\n",
    "        model = LogisticRegression(featuresCol = \"allFeatures\", \n",
    "                                    regParam=param_dict['regParam'], \n",
    "                                    maxIter=param_dict['maxIter'], \n",
    "                                    elasticNetParam=param_dict['elasticNetParam'])\n",
    "    # Build Gradient Boosted Tree model\n",
    "    elif model_name == 'gbt':\n",
    "        model = GBTClassifier(featuresCol = \"allFeatures\",\n",
    "                              maxDepth=param_dict['maxDepth'], # Default: 5\n",
    "                              maxIter=param_dict['maxIter'], # Default: 20\n",
    "                              maxBins=param_dict['maxBins'], # Default: 32\n",
    "                              stepSize=param_dict['stepSize']) # Default: 0.1\n",
    "    # Build Support Vector Machine model\n",
    "    elif model_name == 'svm':\n",
    "        model = LinearSVC(featuresCol=\"scaledFeatures\",\n",
    "                          maxIter=10)\n",
    "    # Build Random Forest model\n",
    "    elif model_name == 'rf':\n",
    "        model = RandomForestClassifier(featuresCol = \"scaledFeatures\",\n",
    "                                       numTrees=70,\n",
    "                                       maxDepth=3, \n",
    "                                       seed=42)\n",
    "    elif model_name == 'mpc':\n",
    "        model = MultilayerPerceptronClassifier(labelCol=\"label\",\n",
    "                                               featuresCol=\"allFeatures\",\n",
    "                                               maxIter=param_dict['maxIter'],\n",
    "                                               layers=param_dict['layers'], \n",
    "                                               stepSize=param_dict['stepSize'],\n",
    "                                               solver=param_dict['solver'],\n",
    "                                               blockSize=param_dict['blockSize'],\n",
    "                                               seed=123)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c44d0112-7003-4861-97ab-9c0bc10b8c7c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Obtain parameters from provided parameter dictionary\n",
    "def udf_grid_search_params(params_grid):\n",
    "    '''\n",
    "    Description:\n",
    "    Input:\n",
    "    Output:\n",
    "    '''\n",
    "    cleaned_dict_list = []\n",
    "\n",
    "    for param_dict in test_output:\n",
    "        cleaned_dict = {param.name: value for param, value in param_dict.items()}\n",
    "        cleaned_dict_list.append(cleaned_dict)\n",
    "    return cleaned_dict_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48f323be-1eed-4caa-8e13-f5544c2e3fe5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create pipline\n",
    "def build_model_pipeline(input_df, input_categoricals_columns, input_pca_columns, input_non_pca_columns, input_prediction_feature = 'model_delay'):\n",
    "    '''\n",
    "    Description:\n",
    "    Input:\n",
    "    Output:\n",
    "    '''\n",
    "    base = input_df\n",
    "\n",
    "    # input the categorical columns\n",
    "    categoricals_columns = input_categoricals_columns\n",
    "\n",
    "    # impute the missing categorical data using the mode\n",
    "    # flight time category was missing data\n",
    "    # we can actuall swtich to the CRS_DEP_TIME to fix this issue, but this is a function that we might have needed anyways or might need in the future\n",
    "    base = impute_categoricals(base,base,categoricals_columns)\n",
    "    \n",
    "    indexers = map(lambda c: StringIndexer(inputCol=c, outputCol=c+\"_idx\", handleInvalid = 'keep'), categoricals_columns)\n",
    "    ohes = map(lambda c: OneHotEncoder(inputCol=c + \"_idx\", outputCol=c+\"_class\"),categoricals_columns)\n",
    "\n",
    "    # Establish features columns\n",
    "    categoricals = list(map(lambda c: c+\"_class\", categoricals_columns))\n",
    "\n",
    "    # input the number columns to be reduced with PCA\n",
    "    numerics_pca_columns = input_pca_columns\n",
    "\n",
    "    # input the columns we decide not to reduce with PCA\n",
    "    numerics_non_pca_columns = input_non_pca_columns\n",
    "\n",
    "    # input the feature we are trying to predict\n",
    "    prediction_feature = input_prediction_feature\n",
    "\n",
    "    all_numerics = numerics_pca_columns + numerics_non_pca_columns\n",
    "\n",
    "    # imputer should handle all numeric columns regardless of usage in pca or not\n",
    "    imputers = Imputer(inputCols = all_numerics, outputCols = all_numerics)\n",
    "\n",
    "    # grab only relevant columns, we need numerics, categorical, and predictor\n",
    "    base = base[all_numerics +categoricals_columns + [prediction_feature]]\n",
    "\n",
    "    # VectorAssembler\n",
    "    assembler_numeric_pca = VectorAssembler( inputCols=numerics_pca_columns, outputCol='features_numeric_pca_pre_scale')\n",
    "    assembler_numeric_non_pca = VectorAssembler( inputCols=numerics_non_pca_columns, outputCol='features_numeric_non_pca_pre_scale')\n",
    "\n",
    "    scaler_non_pca = StandardScaler(inputCol=\"features_numeric_non_pca_pre_scale\",\n",
    "                            outputCol=\"features_numeric_non_pca_scaled\",\n",
    "                            withStd=True,\n",
    "                            withMean=True)\n",
    "\n",
    "    scaler_pca = StandardScaler(inputCol=\"features_numeric_pca_pre_scale\",\n",
    "                            outputCol=\"features_numeric_pca_scaled\",\n",
    "                            withStd=True,\n",
    "                            withMean=True)\n",
    "\n",
    "    pca = PCA(k=2, inputCol='features_numeric_pca_scaled', outputCol='dense_vect_pca_features')\n",
    "\n",
    "    assemblerAll = VectorAssembler(inputCols= [\"features_numeric_non_pca_scaled\", \"dense_vect_pca_features\"] +categoricals , outputCol=\"allFeatures\")\n",
    "    print(assemblerAll.getInputCols)\n",
    "\n",
    "    label = StringIndexer(inputCol=\"model_delay\", outputCol=\"label\")\n",
    "\n",
    "    model_matrix_stages =   list(indexers) + list(ohes) + \\\n",
    "                            [imputers] + \\\n",
    "                            [assembler_numeric_non_pca] + \\\n",
    "                            [assembler_numeric_pca] + \\\n",
    "                            [scaler_non_pca] + [scaler_pca] + \\\n",
    "                            [pca] + \\\n",
    "                            [assemblerAll] + \\\n",
    "                            [label]\n",
    "    return model_matrix_stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61a9e566-fa61-4832-9756-15183b911673",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "## Run time series cross validation and perform hyperparameter tuning to select the best model\n",
    "def tscv(dataset, model, pca_numerics, non_pca_numerics, categoricals, k=5):\n",
    "    '''\n",
    "    Description: Runs time series cross validation on the provided model\n",
    "    Input: Dataset, built model, PCA numeric features, non-PCA numeric features, categorical features, number of folds (k)\n",
    "    Output: Average F1 score of model\n",
    "    '''\n",
    "\n",
    "    # initialize variables \n",
    "    n = dataset.count()\n",
    "    chunk_size = int(n/(k+1))\n",
    "    scores_auc = []\n",
    "    scores_accuracy = []\n",
    "    scores_precision = []\n",
    "    scores_recall = []\n",
    "    scores_f1 = []\n",
    "\n",
    "    # Assume that we are ordering by FL_DATE always \n",
    "    sort_dataset = dataset.withColumn(\"row_id\", F.row_number().over(Window.partitionBy().orderBy(\"FL_DATE\")))\n",
    "\n",
    "    # Perform tscv and hyperparameter tuning \n",
    "    for i in range(k):\n",
    "        train_df = sort_dataset.filter(F.col('row_id') <= chunk_size * (i+1)).cache()\n",
    "        val_df = sort_dataset.filter((F.col('row_id') > chunk_size * (i+1)) & (F.col('row_id') <= chunk_size * (i+2))).cache()\n",
    "\n",
    "        build_pipeline_matrix = build_model_pipeline(train_df, categoricals,pca_numerics,non_pca_numerics,'model_delay')\n",
    "\n",
    "        pipeline = Pipeline (stages=build_pipeline_matrix + [model])\n",
    "\n",
    "        #Train model with train_df\n",
    "        pipeline_model = pipeline.fit(train_df)\n",
    "        \n",
    "        # impute null validation categoricals basked on mode of training categoricals\n",
    "        val_df = impute_categoricals(train_df, val_df, categoricals)\n",
    "\n",
    "        # Predict on validation set\n",
    "        predictions = pipeline_model.transform(val_df).select(\"probability\", \"label\", \"prediction\")\n",
    "        predictions = predictions.rdd.map(extract).toDF([\"p0\", \"p1\", \"label\", \"prediction\"])\n",
    "\n",
    "        valid_input_model = predictions\n",
    "\n",
    "        # Create an evaluator\n",
    "        evaluator_auc = BinaryClassificationEvaluator(labelCol='label')\n",
    "        evaluator_auc.setRawPredictionCol('p1')\n",
    "\n",
    "        # Compute the areaUnderROC on the test data\n",
    "        areaUnderROC = evaluator_auc.evaluate(valid_input_model, {evaluator_auc.metricName: \"areaUnderROC\"})\n",
    "\n",
    "        evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\")\n",
    "\n",
    "        # Compute various evaluation metrics\n",
    "        accuracy = evaluator.evaluate(valid_input_model, {evaluator.metricName: \"accuracy\"})\n",
    "        precision = evaluator.evaluate(valid_input_model, {evaluator.metricName: \"weightedPrecision\"})\n",
    "        recall = evaluator.evaluate(valid_input_model, {evaluator.metricName: \"weightedRecall\"})\n",
    "        f1 = evaluator.evaluate(valid_input_model, {evaluator.metricName: \"f1\"})\n",
    "\n",
    "        print(\"Accuracy = %g\" % accuracy)\n",
    "        print(\"Precision = %g\" % precision)\n",
    "        print(\"Recall = %g\" % recall)\n",
    "        print(\"F1 = %g\" % f1)\n",
    "\n",
    "        # Append the score to the scores list\n",
    "        scores_auc.append(areaUnderROC)\n",
    "        scores_accuracy.append(accuracy)\n",
    "        scores_precision.append(precision)\n",
    "        scores_recall.append(recall)\n",
    "        scores_f1.append(f1)\n",
    "\n",
    "    f1_score = np.mean(scores_f1)\n",
    "    return f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0cea51dc-0d93-4c5c-9beb-9939a7df6194",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_hyperparameters(dataset, model_type, hyperparameter_list, k):\n",
    "    '''\n",
    "    Description:\n",
    "    Input:\n",
    "    Output:\n",
    "    '''\n",
    "    # Initialize variables\n",
    "    best_score = 0\n",
    "    best_param = None\n",
    "    start_time = time.time()\n",
    "    iterations = 0\n",
    "    # Loop through all combinations of hyperparameters and pick the best set\n",
    "    for parameters in hyperparameter_list:\n",
    "        iterations = iterations+1\n",
    "        new_model = build_model(model_type, parameters)\n",
    "        avg_score = tscv(dataset = dataset, model = new_model, k=k)\n",
    "        if avg_score > best_score:\n",
    "            best_score = avg_score\n",
    "            best_params = parameters\n",
    "        print(parameters)\n",
    "        print('best score:', best_score, '|best params:', best_params)\n",
    "        print(\"iteration: \" + str(iterations) + \" --- %s seconds ---\" % (time.time() - start_time))\n",
    "    return best_score, best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54cb48c1-d24f-4182-a9c7-a238a9abbf6f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "## Run time series cross validation and perform hyperparameter tuning to select the best model\n",
    "def single_model_run(dataset, model, pca_numerics, non_pca_numerics, categoricals, k=5):\n",
    "    '''\n",
    "    Description: Runs time series cross validation on the provided model\n",
    "    Input: Dataset, built model, PCA numeric features, non-PCA numeric features, categorical features, number of folds (k)\n",
    "    Output: Average F1 score of model\n",
    "    '''\n",
    "    # initialize variables \n",
    "    # n = dataset.count()\n",
    "    # chunk_size = int(n/(k+1))\n",
    "    scores_auc = []\n",
    "    scores_accuracy = []\n",
    "    scores_precision = []\n",
    "    scores_recall = []\n",
    "    scores_f1 = []\n",
    "\n",
    "    # Assume that we are ordering by FL_DATE always \n",
    "    sort_dataset = dataset.withColumn(\"row_id\", F.row_number().over(Window.partitionBy().orderBy(\"FL_DATE\",F.rand())))\n",
    "\n",
    "    model_n = sort_dataset.count()\n",
    "    split_factor = 0.7\n",
    "\n",
    "    # Perform tscv and hyperparameter tuning \n",
    "\n",
    "    train_df = sort_dataset.filter(F.col('row_id') <= (model_n * split_factor)).cache()\n",
    "    val_df = sort_dataset.filter((F.col('row_id') > (model_n * split_factor))).cache()\n",
    "\n",
    "    build_pipeline_matrix = build_model_pipeline(train_df, categoricals,pca_numerics,non_pca_numerics,'model_delay')\n",
    "\n",
    "    pipeline = Pipeline (stages=build_pipeline_matrix + [model])\n",
    "\n",
    "    #Train model with train_df\n",
    "    pipeline_model = pipeline.fit(train_df)\n",
    "    \n",
    "    # impute null validation categoricals basked on mode of training categoricals\n",
    "    val_df = impute_categoricals(train_df, val_df, categoricals)\n",
    "\n",
    "    # Predict on validation set\n",
    "    predictions = pipeline_model.transform(val_df).select(\"probability\", \"label\", \"prediction\")\n",
    "    predictions = predictions.rdd.map(extract).toDF([\"p0\", \"p1\", \"label\", \"prediction\"])\n",
    "\n",
    "    valid_input_model = predictions\n",
    "\n",
    "    # Create an evaluator\n",
    "    evaluator_auc = BinaryClassificationEvaluator(labelCol='label')\n",
    "    evaluator_auc.setRawPredictionCol('p1')\n",
    "\n",
    "    # Compute the areaUnderROC on the test data\n",
    "    areaUnderROC = evaluator_auc.evaluate(valid_input_model, {evaluator_auc.metricName: \"areaUnderROC\"})\n",
    "\n",
    "    evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\")\n",
    "\n",
    "    # Compute various evaluation metrics\n",
    "    accuracy = evaluator.evaluate(valid_input_model, {evaluator.metricName: \"accuracy\"})\n",
    "    precision = evaluator.evaluate(valid_input_model, {evaluator.metricName: \"weightedPrecision\"})\n",
    "    recall = evaluator.evaluate(valid_input_model, {evaluator.metricName: \"weightedRecall\"})\n",
    "    f1 = evaluator.evaluate(valid_input_model, {evaluator.metricName: \"f1\"})\n",
    "\n",
    "    print(\"Accuracy = %g\" % accuracy)\n",
    "    print(\"Precision = %g\" % precision)\n",
    "    print(\"Recall = %g\" % recall)\n",
    "    print(\"F1 = %g\" % f1)\n",
    "\n",
    "    # Append the score to the scores list\n",
    "    scores_auc.append(areaUnderROC)\n",
    "    scores_accuracy.append(accuracy)\n",
    "    scores_precision.append(precision)\n",
    "    scores_recall.append(recall)\n",
    "    scores_f1.append(f1)\n",
    "\n",
    "    f1_score = np.mean(scores_f1)\n",
    "\n",
    "    return f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "792ca46c-f1a5-40c1-b8cd-b7bf9600c5d4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_hyperparameters_single_run(dataset, model_type, hyperparameter_list, k_folds, pca_numerics, non_pca_numerics, categoricals):\n",
    "    '''\n",
    "    Description:\n",
    "    Input:\n",
    "    Output:\n",
    "    '''\n",
    "    # Initialize variables\n",
    "    best_score = 0\n",
    "    best_param = None\n",
    "\n",
    "    # Loop through all combinations of hyperparameters and pick the best set\n",
    "    for parameters in hyperparameter_list:\n",
    "        new_model = build_model(model_type, parameters)\n",
    "        avg_score = single_model_run(dataset = dataset, model = new_model, pca_numerics = pca_num, non_pca_numerics = non_pca_num, categoricals = categoricals, k = k_folds)\n",
    "        if avg_score > best_score:\n",
    "            best_score = avg_score\n",
    "            best_params = parameters\n",
    "        print(parameters)\n",
    "        print('best score:', best_score, '|best params:', best_params)\n",
    "    return best_score, best_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f1ff42e-fb66-4beb-b07a-82dc63714db7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Run time series cross validation and perform hyperparameter tuning to select the best model\n",
    "def single_model_fit(dataset, model, pca_numerics, non_pca_numerics, categoricals, k=5):\n",
    "    '''\n",
    "    Description: Runs time series cross validation on the provided model\n",
    "    Input: Dataset, built model, PCA numeric features, non-PCA numeric features, categorical features, number of folds (k)\n",
    "    Output: Average F1 score of model\n",
    "    '''\n",
    "    # initialize variables \n",
    "    # n = dataset.count()\n",
    "    # chunk_size = int(n/(k+1))\n",
    "    scores_auc = []\n",
    "    scores_accuracy = []\n",
    "    scores_precision = []\n",
    "    scores_recall = []\n",
    "    scores_f1 = []\n",
    "\n",
    "    # Assume that we are ordering by FL_DATE always \n",
    "    sort_dataset = dataset.withColumn(\"row_id\", F.row_number().over(Window.partitionBy().orderBy(\"FL_DATE\",F.rand())))\n",
    "\n",
    "    model_n = sort_dataset.count()\n",
    "    split_factor = 0.7\n",
    "\n",
    "    # Perform tscv and hyperparameter tuning \n",
    "\n",
    "    train_df = sort_dataset.filter(F.col('row_id') <= (model_n * split_factor)).cache()\n",
    "    val_df = sort_dataset.filter((F.col('row_id') > (model_n * split_factor))).cache()\n",
    "\n",
    "    build_pipeline_matrix = build_model_pipeline(train_df, categoricals,pca_numerics,non_pca_numerics,'model_delay')\n",
    "\n",
    "    pipeline = Pipeline(stages=build_pipeline_matrix + [model])\n",
    "\n",
    "    #Train model with train_df\n",
    "    pipeline_model = pipeline.fit(train_df)\n",
    "\n",
    "    #################### Feature Importances Code ######################\n",
    "\n",
    "    ###################################################################\n",
    "    return pipeline_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b6ab9b1-0f40-4519-ac68-8bdda07f5ab6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def extract(row):\n",
    "    '''\n",
    "    Description:\n",
    "    Input:\n",
    "    Output:\n",
    "    '''\n",
    "    \n",
    "    return tuple(row.probability.toArray().tolist()) +  (row.label,) + (row.prediction,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ea78ecb-2cde-4653-bda6-eddd2eb7776b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# 4. Model Pipeline (Modelling & Hyperparameter Tuning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc40965c-818e-4f12-8a71-e24b113eaa03",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 4.1 Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56eb562d-d09f-49e9-b772-909b45589a8e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# ML Flow Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "072487ac-6711-4c16-8755-994d4678981a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def RegressionEvaluator(preds):\n",
    "    # print(preds)\n",
    "    rdd_preds_m = preds.select(['prediction', 'label']).rdd\n",
    "\n",
    "    # predictions = pipeline_model.transform(df_test_3m).select(\"probability\", \"label\", \"prediction\")\n",
    "    # predictions = predictions.rdd.map(extract).toDF([\"p0\", \"p1\", \"label\", \"prediction\"])\n",
    "\n",
    "    preds = preds.select(\"probability\", \"label\", \"prediction\")\n",
    "    preds = preds.rdd.map(extract).toDF([\"p0\", \"p1\", \"label\", \"prediction\"])\n",
    "\n",
    "\n",
    "    # Create an binary evaluator\n",
    "    evaluator_auc = BinaryClassificationEvaluator(labelCol='label')\n",
    "    evaluator_auc.setRawPredictionCol('p1')\n",
    "\n",
    "    # Compute the areaUnderROC on the test data\n",
    "    areaUnderROC = evaluator_auc.evaluate(preds, {evaluator_auc.metricName: \"areaUnderROC\"})\n",
    "    areaUnderPR = evaluator_auc.evaluate(preds, {evaluator_auc.metricName: \"areaUnderPR\"})\n",
    "\n",
    "    multi_evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\")\n",
    "    multi_evaluator2 = MulticlassMetrics(rdd_preds_m)\n",
    "    # Compute various evaluation metrics\n",
    "    accuracy = multi_evaluator.evaluate(preds, {multi_evaluator.metricName: \"accuracy\"})\n",
    "    precision = multi_evaluator.evaluate(preds, {multi_evaluator.metricName: \"precisionByLabel\"})\n",
    "    recall = multi_evaluator.evaluate(preds, {multi_evaluator.metricName: \"recallByLabel\"})\n",
    "    f1 = multi_evaluator.evaluate(preds, {multi_evaluator.metricName: \"f1\"})\n",
    "\n",
    "    f2 = np.round(multi_evaluator2.fMeasure(label=1.0, beta=2.0), 5)\n",
    "    # pr = binary_evaluator.areaUnderPR\n",
    "\n",
    "    return accuracy, precision, recall, f1, f2 ,areaUnderROC,areaUnderPR\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1df8455-cef8-4020-bdd0-11b16695fb2c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_train = df_train_60m\n",
    "df_test = df_test_60m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b26e320e-e23f-4b72-89ea-9a14f778f789",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'maxIter': <hyperopt.pyll.base.Apply at 0x7f38614280d0>,\n",
       " 'layers': <hyperopt.pyll.base.Apply at 0x7f38614284f0>,\n",
       " 'stepSize': <hyperopt.pyll.base.Apply at 0x7f3861428310>,\n",
       " 'solver': <hyperopt.pyll.base.Apply at 0x7f38614281f0>,\n",
       " 'blockSize': <hyperopt.pyll.base.Apply at 0x7f3861428070>}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CHANGES HERE\n",
    "from hyperopt import hp\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "\n",
    "model_nn = MultilayerPerceptronClassifier(labelCol=\"label\", featuresCol=\"allFeatures\", seed=123)\n",
    "pipeline = Pipeline(stages=[model_nn])\n",
    "feature_size = df_train.schema[\"allFeatures\"].metadata[\"ml_attr\"][\"num_attrs\"]\n",
    "\n",
    "## String format : MLP - 37 - 4 Relu - 2 Softmax; MLP - 37 - 4 Relu - 2 relu- 2 Softmax\n",
    "# search_space = {\n",
    "#     \"maxIter\": hp.quniform(\"maxIter\", 50, 200, 25),\n",
    "#     # \"layers\": hp.choice('layers', [[feature_size, 4, 2], [feature_size, 4, 2, 2]]),\n",
    "#     \"layers\": hp.choice('layers', [[feature_size, 4, 2, 2], [feature_size, 4, 2, 2]]),\n",
    "#     \"stepSize\": hp.quniform(\"stepSize\", 0.1, 1, 0.1),\n",
    "#     \"solver\": hp.choice('solver', ['gd', 'l-bfgs']),\n",
    "#     'blockSize': hp.quniform('blockSize', 16, 128, 16)\n",
    "# }\n",
    "\n",
    "\n",
    "## String format : MLP - 37 - 4 Relu - 2 Softmax; MLP - 37 - 32 Relu - 16 relu - 2 Softmax; MLP - 37 - 2 Relu - 1 relu - 2 Softmax \n",
    "# search_space = {\n",
    "#     \"maxIter\": hp.quniform(\"maxIter\", 175, 200, 25),\n",
    "#     \"layers\": hp.choice('layers', [[feature_size, 4, 2, 2], [feature_size, 32, 16, 2], [feature_size, 2, 1, 2]]),# maybe throw away\n",
    "#     \"stepSize\": hp.quniform(\"stepSize\", 0.7, 1, 0.1),\n",
    "#     \"solver\": hp.choice('solver', ['l-bfgs']),\n",
    "#     'blockSize': hp.quniform('blockSize', 96, 128, 16)\n",
    "# }\n",
    "\n",
    "search_space = {\n",
    "    \"maxIter\": hp.choice(\"maxIter\", [175]),\n",
    "    \"layers\": hp.choice('layers', [[feature_size, 32, 16, 2]]),# maybe throw away\n",
    "    \"stepSize\": hp.choice(\"stepSize\", [0.8]),\n",
    "    \"solver\": hp.choice('solver', ['l-bfgs']),\n",
    "    'blockSize': hp.choice('blockSize', [96])\n",
    "}\n",
    "\n",
    "search_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70030848-552b-487f-8b6c-17c43465535d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def objective_function(params):\n",
    "    # CHANGES HERE\n",
    "    maxIter = params[\"maxIter\"]\n",
    "    layers = params[\"layers\"]\n",
    "    stepSize = params[\"stepSize\"]\n",
    "    solver = params[\"solver\"]\n",
    "    blockSize = params[\"blockSize\"]\n",
    "\n",
    "\n",
    "    with mlflow.start_run():\n",
    "        # CHANGES HERE\n",
    "        estimator = pipeline.copy({model_nn.maxIter:maxIter,\n",
    "                                  model_nn.layers:layers,\n",
    "                                  model_nn.stepSize:stepSize,\n",
    "                                  model_nn.solver:solver,\n",
    "                                  model_nn.blockSize:blockSize})\n",
    "        \n",
    "\n",
    "        model = estimator.fit(df_train)\n",
    "\n",
    "        preds_training = model.transform(df_train)       \n",
    "        pred_calc_training = RegressionEvaluator(preds_training)  \n",
    "\n",
    "        train_accuracy = pred_calc_training[0]\n",
    "        train_precision = pred_calc_training[1]\n",
    "        train_recall = pred_calc_training[2]\n",
    "        train_f1_score = pred_calc_training[3]\n",
    "        train_f2_score = pred_calc_training[4]\n",
    "        train_areaUnderROC = pred_calc_training[5]\n",
    "        train_areaUnderPR = pred_calc_training[6]\n",
    "\n",
    "        mlflow.log_metric('train_accuracy', train_accuracy)\n",
    "        mlflow.log_metric('train_precision', train_precision)\n",
    "        mlflow.log_metric('train_recall', train_recall)\n",
    "        mlflow.log_metric('train_f1_score', train_f1_score)\n",
    "        mlflow.log_metric('train_f2_score', train_f2_score)\n",
    "        mlflow.log_metric('train_areaUnderROC', train_areaUnderROC)\n",
    "        mlflow.log_metric('train_areaUnderPR', train_areaUnderPR)\n",
    "\n",
    "\n",
    "        print('-------------------')\n",
    "        print('Train Metrics:')\n",
    "        print('accuracy:',train_accuracy)\n",
    "        print('precision:',train_precision)\n",
    "        print('recall:',train_recall)\n",
    "\n",
    "        print('f1_score:',train_f1_score)\n",
    "        print('f2_score:',train_f2_score)\n",
    "\n",
    "        print('areaUnderROC:',str(train_areaUnderROC))\n",
    "        print('areaUnderPR:',str(train_areaUnderPR))\n",
    "\n",
    "        preds = model.transform(df_val)\n",
    "        pred_calc = RegressionEvaluator(preds)\n",
    "        val_accuracy = pred_calc[0]\n",
    "        val_precision = pred_calc[1]\n",
    "        val_recall = pred_calc[2]\n",
    "        val_f1_score = pred_calc[3]\n",
    "        val_f2_score = pred_calc[4]\n",
    "        val_areaUnderROC = pred_calc[5]\n",
    "        val_areaUnderPR = pred_calc[6]\n",
    "\n",
    "        mlflow.log_metric('val_accuracy', val_accuracy)\n",
    "        mlflow.log_metric('val_precision', val_precision)\n",
    "        mlflow.log_metric('val_recall', val_recall)\n",
    "        mlflow.log_metric('val_f1_score', val_f1_score)\n",
    "        mlflow.log_metric('val_f2_score', val_f2_score)\n",
    "        mlflow.log_metric('val_areaUnderROC', val_areaUnderROC)\n",
    "        mlflow.log_metric('val_areaUnderPR', val_areaUnderPR)\n",
    "        print('-------------------')\n",
    "        print('Validation Metrics:')\n",
    "        print('accuracy:',val_accuracy)\n",
    "        print('precision:',val_precision)\n",
    "        print('recall:',val_recall)\n",
    "        print('f1_score:',val_f1_score)\n",
    "        print('f2_score:',val_f2_score)\n",
    "        print('areaUnderROC:',val_areaUnderROC)\n",
    "        print('areaUnderPR:',val_areaUnderPR)\n",
    "\n",
    "\n",
    "        print('-------------------')\n",
    "        print('Model Params:')\n",
    "        print('maxIter:',maxIter)\n",
    "        print('layers:',layers)\n",
    "        print('stepSize:',stepSize)\n",
    "        print('solver:',solver)\n",
    "        print('blockSize:',blockSize)\n",
    "\n",
    "        mlflow.spark.log_model(model, \"NN_Model_Test_60m\")\n",
    "        # print('model_logging_complete' + \" --- %s seconds ---\" % (time.time() - start_time))\n",
    "    return val_areaUnderPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f3b692f-fe72-4e8d-84e8-a933dbcaf8d7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# def objective_function(params):\n",
    "#     # CHANGES HERE\n",
    "#     maxIter = params[\"maxIter\"]\n",
    "#     layers = params[\"layers\"]\n",
    "#     stepSize = params[\"stepSize\"]\n",
    "#     solver = params[\"solver\"]\n",
    "#     blockSize = params[\"blockSize\"]\n",
    "\n",
    "#     start_time = time.time()\n",
    "\n",
    "#     with mlflow.start_run():\n",
    "#         # CHANGES HERE\n",
    "#         estimator = pipeline.copy({model_nn.maxIter:maxIter,\n",
    "#                                   model_nn.layers:layers,\n",
    "#                                   model_nn.stepSize:stepSize,\n",
    "#                                   model_nn.solver:solver,\n",
    "#                                   model_nn.blockSize:blockSize})\n",
    "#         model = estimator.fit(df_train)\n",
    "#         # print('fit_complete'+ \" --- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "#         preds = model.transform(df_test)\n",
    "#         # print('transform_complete' + \" --- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "#         pred_calc = RegressionEvaluator(preds)\n",
    "#         # print('evaluation_metrics_complete' + \" --- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "#         mlflow.spark.log_model(model, \"NN_Model_1YEAR\")\n",
    "#         print('model_logging_complete' + \" --- %s seconds ---\" % (time.time() - start_time))\n",
    "#         f1_score = pred_calc[3]\n",
    "#         f2_score = pred_calc[4]\n",
    "#         mlflow.log_metric('f1', f1_score)\n",
    "\n",
    "#         print(f1_score,maxIter,layers,stepSize,solver,blockSize)\n",
    "#     return f2_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0adb2886-d237-4c1c-b6d8-d1f898a416e5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r  0%|          | 0/1 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/08/08 20:57:06 WARNING mlflow.data.spark_dataset: Failed to infer schema for Spark dataset. Exception: Unsupported Spark Type '<class 'pyspark.ml.linalg.VectorUDT'>', MLflow schema is only supported for scalar Spark types.\n\n/databricks/spark/python/pyspark/sql/context.py:165: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n  warnings.warn(\n\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r                                                     \r-------------------\n\r  0%|          | 0/1 [41:32<?, ?trial/s, best loss=?]\r                                                     \rTrain Metrics:\n\r  0%|          | 0/1 [41:32<?, ?trial/s, best loss=?]\r                                                     \raccuracy:\n\r  0%|          | 0/1 [41:32<?, ?trial/s, best loss=?]\r                                                     \r0.8226789893561179\n\r  0%|          | 0/1 [41:32<?, ?trial/s, best loss=?]\r                                                     \rprecision:\n\r  0%|          | 0/1 [41:32<?, ?trial/s, best loss=?]\r                                                     \r0.825025096299883\n\r  0%|          | 0/1 [41:32<?, ?trial/s, best loss=?]\r                                                     \rrecall:\n\r  0%|          | 0/1 [41:32<?, ?trial/s, best loss=?]\r                                                     \r0.9952883140576727\n\r  0%|          | 0/1 [41:32<?, ?trial/s, best loss=?]\r                                                     \rf1_score:\n\r  0%|          | 0/1 [41:32<?, ?trial/s, best loss=?]\r                                                     \r0.7505778612833166\n\r  0%|          | 0/1 [41:32<?, ?trial/s, best loss=?]\r                                                     \rf2_score:\n\r  0%|          | 0/1 [41:32<?, ?trial/s, best loss=?]\r                                                     \r0.03357\n\r  0%|          | 0/1 [41:32<?, ?trial/s, best loss=?]\r                                                     \rareaUnderROC:\n\r  0%|          | 0/1 [41:32<?, ?trial/s, best loss=?]\r                                                     \r0.6899653876163054\n\r  0%|          | 0/1 [41:32<?, ?trial/s, best loss=?]\r                                                     \rareaUnderPR:\n\r  0%|          | 0/1 [41:32<?, ?trial/s, best loss=?]\r                                                     \r0.3262883575881441\n\r  0%|          | 0/1 [41:32<?, ?trial/s, best loss=?]\r                                                     \r-------------------\n\r  0%|          | 0/1 [43:06<?, ?trial/s, best loss=?]\r                                                     \rValidation Metrics:\n\r  0%|          | 0/1 [43:06<?, ?trial/s, best loss=?]\r                                                     \raccuracy:\n\r  0%|          | 0/1 [43:06<?, ?trial/s, best loss=?]\r                                                     \r0.8200975933511473\n\r  0%|          | 0/1 [43:06<?, ?trial/s, best loss=?]\r                                                     \rprecision:\n\r  0%|          | 0/1 [43:06<?, ?trial/s, best loss=?]\r                                                     \r0.8231758204390788\n\r  0%|          | 0/1 [43:06<?, ?trial/s, best loss=?]\r                                                     \rrecall:\n\r  0%|          | 0/1 [43:06<?, ?trial/s, best loss=?]\r                                                     \r0.99382950834628\n\r  0%|          | 0/1 [43:06<?, ?trial/s, best loss=?]\r                                                     \rf1_score:\n\r  0%|          | 0/1 [43:06<?, ?trial/s, best loss=?]\r                                                     \r0.7490557393016876\n\r  0%|          | 0/1 [43:06<?, ?trial/s, best loss=?]\r                                                     \rf2_score:\n\r  0%|          | 0/1 [43:06<?, ?trial/s, best loss=?]\r                                                     \r0.04162\n\r  0%|          | 0/1 [43:06<?, ?trial/s, best loss=?]\r                                                     \rareaUnderROC:\n\r  0%|          | 0/1 [43:06<?, ?trial/s, best loss=?]\r                                                     \r0.6900152264130667\n\r  0%|          | 0/1 [43:06<?, ?trial/s, best loss=?]\r                                                     \rareaUnderPR:\n\r  0%|          | 0/1 [43:06<?, ?trial/s, best loss=?]\r                                                     \r0.33109242467464406\n\r  0%|          | 0/1 [43:06<?, ?trial/s, best loss=?]\r                                                     \r-------------------\n\r  0%|          | 0/1 [43:06<?, ?trial/s, best loss=?]\r                                                     \rModel Params:\n\r  0%|          | 0/1 [43:06<?, ?trial/s, best loss=?]\r                                                     \rmaxIter:\n\r  0%|          | 0/1 [43:06<?, ?trial/s, best loss=?]\r                                                     \r175\n\r  0%|          | 0/1 [43:06<?, ?trial/s, best loss=?]\r                                                     \rlayers:\n\r  0%|          | 0/1 [43:06<?, ?trial/s, best loss=?]\r                                                     \r(37, 32, 16, 2)\n\r  0%|          | 0/1 [43:06<?, ?trial/s, best loss=?]\r                                                     \rstepSize:\n\r  0%|          | 0/1 [43:06<?, ?trial/s, best loss=?]\r                                                     \r0.8\n\r  0%|          | 0/1 [43:06<?, ?trial/s, best loss=?]\r                                                     \rsolver:\n\r  0%|          | 0/1 [43:06<?, ?trial/s, best loss=?]\r                                                     \rl-bfgs\n\r  0%|          | 0/1 [43:06<?, ?trial/s, best loss=?]\r                                                     \rblockSize:\n\r  0%|          | 0/1 [43:06<?, ?trial/s, best loss=?]\r                                                     \r96\n\r  0%|          | 0/1 [43:06<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/08/08 21:40:17 INFO mlflow.spark: Inferring pip requirements by reloading the logged model from the databricks artifact repository, which can be time-consuming. To speed up, explicitly specify the conda_env or pip_requirements when calling log_model().\n\n/databricks/python/lib/python3.10/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\n  warnings.warn(\"Setuptools is replacing distutils.\")\n\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r100%|██████████| 1/1 [43:57<00:00, 2637.70s/trial, best loss: 0.33109242467464406]\r100%|██████████| 1/1 [43:57<00:00, 2637.70s/trial, best loss: 0.33109242467464406]\n"
     ]
    }
   ],
   "source": [
    "from hyperopt import fmin, tpe, Trials\n",
    "import mlflow\n",
    "\n",
    "mlflow.pyspark.ml.autolog(log_models=False)\n",
    "num_evals = 1\n",
    "trials = Trials()\n",
    "best_hyperparam = fmin(fn=objective_function,\n",
    "                       space = search_space,\n",
    "                       algo=tpe.suggest,\n",
    "                       max_evals = num_evals,\n",
    "                       trials=trials,\n",
    "                       rstate=np.random.default_rng(42))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5701abe-bbcc-46af-9d7c-ee16dd0384f4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'blockSize': 0, 'layers': 0, 'maxIter': 0, 'solver': 0, 'stepSize': 0}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_hyperparam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "888f5079-ebc2-4fc6-828d-8bd4ed7aaddb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "MultilayerPerceptronClassifier_c41af4305c0c"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MultilayerPerceptronClassifier()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [
    {
     "elements": [],
     "globalVars": {},
     "guid": "60d14487-c252-4ac3-a964-b4567397751e",
     "layoutOption": {
      "grid": true,
      "stack": true
     },
     "nuid": "d404ab4d-79db-4b22-8327-4df01cb5dc02",
     "origId": 3984215836277737,
     "title": "Untitled",
     "version": "DashboardViewV1",
     "width": 1024
    },
    {
     "elements": [],
     "globalVars": {},
     "guid": "d6f2fdd3-30ae-44b4-acb3-f9220f25471e",
     "layoutOption": {
      "grid": true,
      "stack": true
     },
     "nuid": "e5336f6e-92ad-4e64-8bc1-efa53b3cc9cb",
     "origId": 3984215836277738,
     "title": "Summary Table",
     "version": "DashboardViewV1",
     "width": 1024
    }
   ],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "FP_Section2_Group4_NN_60month",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
