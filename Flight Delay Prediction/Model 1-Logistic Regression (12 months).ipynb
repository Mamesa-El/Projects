{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc4b6aae-1737-4047-8aa3-a7d3a56996a7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Loading in Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f35f3799-f835-4904-ac3f-635f9811dc4b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.databricks.v1+bamboolib_hint": "{\"pd.DataFrames\": [], \"version\": \"0.0.1\"}",
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Standard libraries/functions\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Pyspark libraries/functions\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import col, sum, isnan, when, count, year, month, dayofmonth, date_format, concat_ws, acos, cos, radians, sin, udf, concat\n",
    "from pyspark.sql.types import IntegerType, DateType, DoubleType, StringType, FloatType\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# from pyspark.ml.linalg import Vectors#, VectorUDT\n",
    "# from pyspark.ml.linalg import VectorType\n",
    "from pyspark.mllib.linalg import Vectors, VectorUDT\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler, StringIndexer, OneHotEncoder, Imputer\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import PCA\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics, MulticlassMetrics\n",
    "\n",
    "# Sklearn libraries/functions\n",
    "from sklearn.utils import shuffle\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from pyspark.ml.classification import LogisticRegression, GBTClassifier, RandomForestClassifier, LinearSVC\n",
    "import time\n",
    "\n",
    "import mlflow\n",
    "import mlflow.spark\n",
    "# from pyspark.mllib.linalg import Vectors, VectorUDT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6dc1c5f0-7401-44bd-b399-45721ce659a8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## library settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f7ca007-066a-42e6-acdd-58b8b67f35fe",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Setting Up Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84be5c3b-a766-4381-ab12-1e72d2c513de",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>wasbs://smsj-261@smsj.blob.core.windows.net/test/</td><td>test/</td><td>0</td><td>1689534418000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "wasbs://smsj-261@smsj.blob.core.windows.net/test/",
         "test/",
         0,
         1689534418000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "size",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "modificationTime",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Place this cell in any team notebook that needs access to the team cloud storage.\n",
    "\n",
    "# The following blob storage is accessible to team members only (read and write)\n",
    "# access key is valid til TTL\n",
    "# after that you will need to create a new SAS key and authenticate access again via DataBrick command line\n",
    "blob_container  = \"smsj-261\"       # The name of your container created in https://portal.azure.com\n",
    "storage_account = \"smsj\"  # The name of your Storage account created in https://portal.azure.com\n",
    "secret_scope    = \"smsjscope\"           # The name of the scope created in your local computer using the Databricks CLI\n",
    "secret_key      = \"smsjkey\"             # The name of the secret key created in your local computer using the Databricks CLI\n",
    "team_blob_url        = f\"wasbs://{blob_container}@{storage_account}.blob.core.windows.net\"  #points to the root of your team storage bucket\n",
    "\n",
    "# the 261 course blob storage is mounted here on the DataBricks workspace.\n",
    "mids261_mount_path      = \"/mnt/mids-w261\"\n",
    "\n",
    "# SAS Token: Grant the team limited access to Azure Storage resources\n",
    "spark.conf.set(\n",
    "  f\"fs.azure.sas.{blob_container}.{storage_account}.blob.core.windows.net\",\n",
    "  dbutils.secrets.get(scope = secret_scope, key = secret_key)\n",
    ")\n",
    "import pandas as pd\n",
    "pdf = pd.DataFrame([[1, 2, 3, \"Jane\"], [2, 2,2, None], [12, 12,12, \"John\"]], columns=[\"x\", \"y\", \"z\", \"a_string\"])\n",
    "df = spark.createDataFrame(pdf) # Create a Spark dataframe from a pandas DF\n",
    "\n",
    "# The following can write the dataframe to the team's Cloud Storage  \n",
    "# Navigate back to your Storage account in https://portal.azure.com, to inspect the partitions/files.\n",
    "# df.write.parquet(f\"{team_blob_url}/test\")\n",
    "\n",
    "# see what's in the blob storage root folder \n",
    "display(dbutils.fs.ls(f\"{team_blob_url}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7055ebcf-6d00-4fee-9b39-434aef06a0b6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "afa6cfda-6a99-4ea2-873a-e17162e8fac2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# UPLOADING 3 MONTH DATASET\n",
    "# df_test_3m = spark.sql(\"SELECT * FROM test_3m\")\n",
    "\n",
    "# df_train_3m = spark.sql(\"SELECT * FROM train_3m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41295f59-f792-4fcd-852a-fb98e216799a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# import spark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"ReadingParquet\").getOrCreate()\n",
    "\n",
    "# df = spark.read.parquet(\"/path/to/parquet/file.parquet\")\n",
    "# df.show()\n",
    "\n",
    "# spark = spark.sql.SparkSession.builder.appName(\"Call Parquet File\").getOrCreate()\n",
    "\n",
    "# df_test_3m = spark.read.parquet(\"dbfs:/user/hive/warehouse/test_3m\")\n",
    "# df_train_3m = spark.read.parquet(\"dbfs:/user/hive/warehouse/train_3m\")\n",
    "\n",
    "df_test_3m = spark.read.parquet(\"dbfs:/user/hive/warehouse/test_3m\")\n",
    "df_train_3m = spark.read.parquet(\"dbfs:/user/hive/warehouse/train_3m\")\n",
    "\n",
    "df_test_12m = spark.read.parquet(\"dbfs:/user/hive/warehouse/test_12m\")\n",
    "df_train_12m = spark.read.parquet(\"dbfs:/user/hive/warehouse/train_12m\")\n",
    "\n",
    "df_test_60m = spark.read.parquet(\"dbfs:/user/hive/warehouse/test_60m\")\n",
    "df_train_60m = spark.read.parquet(\"dbfs:/user/hive/warehouse/train_60m\")\n",
    "\n",
    "# df = spark.read.parquet(\"s3://my-bucket/test_3m.parquet\")\n",
    "\n",
    "# df_test_3m.createOrReplaceTempView(\"test_3m\")\n",
    "# df_train_3m.createOrReplaceTempView(\"df_train_3m\")\n",
    "\n",
    "# df_test_12m.createOrReplaceTempView(\"test_12m\")\n",
    "# df_train_12m.createOrReplaceTempView(\"df_train_12m\")\n",
    "\n",
    "# Use the DataFrame in your ML Flow project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "832818ab-aae0-46f1-b811-e3d96686fd93",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType, DoubleType, IntegerType\n",
    "\n",
    "def total_count(df_train, df_test, months):\n",
    "    # Concatenate the train and test dataframes to get the combined dataframe\n",
    "    df = df_train.union(df_test)\n",
    "\n",
    "    # Identify numerical and categorical columns\n",
    "    numerical_features = [f.name for f in df.schema.fields if isinstance(f.dataType, (DoubleType, IntegerType))]\n",
    "    categorical_features = [f.name for f in df.schema.fields if isinstance(f.dataType, StringType)]\n",
    "\n",
    "    # Get the counts\n",
    "    total_rows = df.count()\n",
    "    total_columns = len(df.columns)\n",
    "    total_num_features = len(numerical_features)\n",
    "    total_cat_features = len(categorical_features)\n",
    "\n",
    "    # Print results\n",
    "    print(months, \"Total rows:\", total_rows)\n",
    "    print(months, \"Total columns:\", total_columns)\n",
    "    print(months, \"Total numerical features:\", total_num_features)\n",
    "    print(months, \"Total categorical features:\", total_cat_features)\n",
    "    print(\"------------------------------------------\")\n",
    "\n",
    "# Sample usage\n",
    "# Assume df_train and df_test are PySpark DataFrames you've defined earlier\n",
    "# total_count(df_train, df_test, \"August\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1fafe7fa-da42-4920-8c7e-975ed6e8dbd0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 months Total rows: 1401363\n3 months Total columns: 103\n3 months Total numerical features: 41\n3 months Total categorical features: 25\n------------------------------------------\n12 months Total rows: 5811854\n12 months Total columns: 109\n12 months Total numerical features: 42\n12 months Total categorical features: 23\n------------------------------------------\n60 months Total rows: 12926912\n60 months Total columns: 109\n60 months Total numerical features: 42\n60 months Total categorical features: 23\n------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "total_count (df_train_3m, df_test_3m, \"3 months\")\n",
    "total_count (df_train_12m, df_test_12m, \"12 months\")\n",
    "total_count (df_train_60m, df_test_60m, \"60 months\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62fd3578-41f2-42b1-aa9e-607b1f1571ea",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Initializing Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c80ce5cb-c08b-4ef4-8c42-ab5dfb4524d6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Modelling Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e666118f-5ad0-480b-89d3-1553a86dd56d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def build_model (model_name, param_dict):\n",
    "    '''\n",
    "    Description: Contains models and hyperparameters\n",
    "    Inputs:\n",
    "    Outputs: \n",
    "    '''\n",
    "    # Build Logistic Regression model\n",
    "    if model_name == 'log':\n",
    "        model = LogisticRegression(featuresCol = \"allFeatures\", \n",
    "                                    regParam=param_dict['regParam'], \n",
    "                                    maxIter=param_dict['maxIter'], \n",
    "                                    elasticNetParam=param_dict['elasticNetParam'])\n",
    "    # Build Gradient Boosted Tree model\n",
    "    elif model_name == 'gbt':\n",
    "        model = GBTClassifier(featuresCol = \"allFeatures\",\n",
    "                              maxDepth=param_dict['maxDepth'], # Default: 5\n",
    "                              maxIter=param_dict['maxIter'], # Default: 20\n",
    "                              maxBins=param_dict['maxBins'], # Default: 32\n",
    "                              stepSize=param_dict['stepSize']) # Default: 0.1\n",
    "    # Build Support Vector Machine model\n",
    "    elif model_name == 'svm':\n",
    "        model = LinearSVC(featuresCol=\"scaledFeatures\",\n",
    "                          maxIter=10)\n",
    "    # Build Random Forest model\n",
    "    elif model_name == 'rf':\n",
    "        model = RandomForestClassifier(featuresCol = \"scaledFeatures\",\n",
    "                                       numTrees=70,\n",
    "                                       maxDepth=3, \n",
    "                                       seed=42)\n",
    "    return model\n",
    "\n",
    "\n",
    "# IMPROVED VERSION TO TEST:\n",
    "# from pyspark.ml.classification import (LogisticRegression, GBTClassifier, \n",
    "#     LinearSVC, \n",
    "#     RandomForestClassifier\n",
    "# )\n",
    "\n",
    "# def build_model(model_name, param_dict):\n",
    "#     \"\"\"\n",
    "#     Description: Contains models and hyperparameters\n",
    "#     Inputs:\n",
    "#     Outputs: \n",
    "#     \"\"\"\n",
    "#     model_class_mapping = {\n",
    "#         'log': LogisticRegression,\n",
    "#         'gbt': GBTClassifier,\n",
    "#         'svm': LinearSVC,\n",
    "#         'rf': RandomForestClassifier\n",
    "#     }\n",
    "\n",
    "#     model_class = model_class_mapping.get(model_name)\n",
    "#     if model_class is None:\n",
    "#         raise ValueError(f\"Model '{model_name}' is not supported.\")\n",
    "\n",
    "#     model = model_class(featuresCol=\"allFeatures\", **param_dict)\n",
    "#     return model\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c44d0112-7003-4861-97ab-9c0bc10b8c7c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Obtain parameters from provided parameter dictionary\n",
    "def udf_grid_search_params(params_grid):\n",
    "    '''\n",
    "    Description:\n",
    "    Input:\n",
    "    Output:\n",
    "    '''\n",
    "    cleaned_dict_list = []\n",
    "\n",
    "    for param_dict in test_output:\n",
    "        cleaned_dict = {param.name: value for param, value in param_dict.items()}\n",
    "        cleaned_dict_list.append(cleaned_dict)\n",
    "    return cleaned_dict_list\n",
    "\n",
    "\n",
    "# IMPROVED VERSION TO TEST:\n",
    "# def udf_grid_search_params(params_grid):\n",
    "#     \"\"\"\n",
    "#     Description: Extract parameters from the provided parameter dictionary\n",
    "#     Input: params_grid - List of dictionaries representing different parameter combinations\n",
    "#     Output: List of dictionaries with cleaned parameters\n",
    "#     \"\"\"\n",
    "#     cleaned_dict_list = [{param.name: value for param, value in param_dict.items()} for param_dict in params_grid]\n",
    "#     return cleaned_dict_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48f323be-1eed-4caa-8e13-f5544c2e3fe5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create pipline\n",
    "def build_model_pipeline(input_df, input_categoricals_columns, input_pca_columns, input_non_pca_columns, input_prediction_feature = 'model_delay'):\n",
    "    '''\n",
    "    Description:\n",
    "    Input:\n",
    "    Output:\n",
    "    '''\n",
    "    base = input_df\n",
    "\n",
    "    # input the categorical columns\n",
    "    categoricals_columns = input_categoricals_columns\n",
    "\n",
    "    # impute the missing categorical data using the mode\n",
    "    # flight time category was missing data\n",
    "    # we can actuall swtich to the CRS_DEP_TIME to fix this issue, but this is a function that we might have needed anyways or might need in the future\n",
    "    base = impute_categoricals(base,base,categoricals_columns)\n",
    "    \n",
    "    indexers = map(lambda c: StringIndexer(inputCol=c, outputCol=c+\"_idx\", handleInvalid = 'keep'), categoricals_columns)\n",
    "    ohes = map(lambda c: OneHotEncoder(inputCol=c + \"_idx\", outputCol=c+\"_class\"),categoricals_columns)\n",
    "\n",
    "    # Establish features columns\n",
    "    categoricals = list(map(lambda c: c+\"_class\", categoricals_columns))\n",
    "\n",
    "    # input the number columns to be reduced with PCA\n",
    "    numerics_pca_columns = input_pca_columns\n",
    "\n",
    "    # input the columns we decide not to reduce with PCA\n",
    "    numerics_non_pca_columns = input_non_pca_columns\n",
    "\n",
    "    # input the feature we are trying to predict\n",
    "    prediction_feature = input_prediction_feature\n",
    "\n",
    "    all_numerics = numerics_pca_columns + numerics_non_pca_columns\n",
    "\n",
    "    # imputer should handle all numeric columns regardless of usage in pca or not\n",
    "    imputers = Imputer(inputCols = all_numerics, outputCols = all_numerics)\n",
    "\n",
    "    # grab only relevant columns, we need numerics, categorical, and predictor\n",
    "    base = base[all_numerics +categoricals_columns + [prediction_feature]]\n",
    "\n",
    "    # VectorAssembler\n",
    "    assembler_numeric_pca = VectorAssembler( inputCols=numerics_pca_columns, outputCol='features_numeric_pca_pre_scale')\n",
    "    assembler_numeric_non_pca = VectorAssembler( inputCols=numerics_non_pca_columns, outputCol='features_numeric_non_pca_pre_scale')\n",
    "\n",
    "    scaler_non_pca = StandardScaler(inputCol=\"features_numeric_non_pca_pre_scale\",\n",
    "                            outputCol=\"features_numeric_non_pca_scaled\",\n",
    "                            withStd=True,\n",
    "                            withMean=True)\n",
    "\n",
    "    scaler_pca = StandardScaler(inputCol=\"features_numeric_pca_pre_scale\",\n",
    "                            outputCol=\"features_numeric_pca_scaled\",\n",
    "                            withStd=True,\n",
    "                            withMean=True)\n",
    "\n",
    "    pca = PCA(k=2, inputCol='features_numeric_pca_scaled', outputCol='dense_vect_pca_features')\n",
    "\n",
    "    assemblerAll = VectorAssembler(inputCols= [\"features_numeric_non_pca_scaled\", \"dense_vect_pca_features\"] +categoricals , outputCol=\"allFeatures\")\n",
    "    print(assemblerAll.getInputCols)\n",
    "\n",
    "    label = StringIndexer(inputCol=\"model_delay\", outputCol=\"label\")\n",
    "\n",
    "    model_matrix_stages =   list(indexers) + list(ohes) + \\\n",
    "                            [imputers] + \\\n",
    "                            [assembler_numeric_non_pca] + \\\n",
    "                            [assembler_numeric_pca] + \\\n",
    "                            [scaler_non_pca] + [scaler_pca] + \\\n",
    "                            [pca] + \\\n",
    "                            [assemblerAll] + \\\n",
    "                            [label]\n",
    "    return model_matrix_stages\n",
    "\n",
    "\n",
    "# IMPROVED VERSION TO TEST:\n",
    "# from pyspark.ml.feature import StringIndexer, OneHotEncoder, Imputer, VectorAssembler, StandardScaler, PCA\n",
    "# from pyspark.ml import Pipeline\n",
    "\n",
    "# def build_model_pipeline(input_df, input_categoricals_columns, input_pca_columns, input_non_pca_columns, input_prediction_feature='model_delay'):\n",
    "#     '''\n",
    "#     Description: Create a pipeline for building the model matrix\n",
    "#     Input:\n",
    "#     Output: List of stages for the model matrix pipeline\n",
    "#     '''\n",
    "#     base_df = input_df\n",
    "\n",
    "#     # Input the categorical columns\n",
    "#     categorical_columns = input_categoricals_columns\n",
    "\n",
    "#     # Impute the missing categorical data using the mode\n",
    "#     # Flight time category was missing data\n",
    "#     # We can actually switch to the CRS_DEP_TIME to fix this issue, but this is a function that we might have needed anyways or might need in the future\n",
    "#     base_df = impute_categoricals(base_df, base_df, categorical_columns)\n",
    "\n",
    "#     # Create stages for StringIndexer and OneHotEncoder\n",
    "#     indexers = [StringIndexer(inputCol=col, outputCol=f\"{col}_idx\", handleInvalid='keep') for col in categorical_columns]\n",
    "#     ohes = [OneHotEncoder(inputCol=f\"{col}_idx\", outputCol=f\"{col}_class\") for col in categorical_columns]\n",
    "\n",
    "#     # Establish feature columns\n",
    "#     categorical_features = [f\"{col}_class\" for col in categorical_columns]\n",
    "\n",
    "#     # Input the numeric columns to be reduced with PCA\n",
    "#     numeric_pca_columns = input_pca_columns\n",
    "\n",
    "#     # Input the columns we decide not to reduce with PCA\n",
    "#     numeric_non_pca_columns = input_non_pca_columns\n",
    "\n",
    "#     # Input the feature we are trying to predict\n",
    "#     prediction_feature = input_prediction_feature\n",
    "\n",
    "#     all_numeric_columns = numeric_pca_columns + numeric_non_pca_columns\n",
    "\n",
    "#     # Imputer should handle all numeric columns regardless of usage in PCA or not\n",
    "#     imputer = Imputer(inputCols=all_numeric_columns, outputCols=all_numeric_columns)\n",
    "\n",
    "#     # Grab only relevant columns - numerics, categorical, and predictor\n",
    "#     base_df = base_df[all_numeric_columns + categorical_columns + [prediction_feature]]\n",
    "\n",
    "#     # Create VectorAssembler for numeric PCA and non-PCA features\n",
    "#     assembler_numeric_pca = VectorAssembler(inputCols=numeric_pca_columns, outputCol='features_numeric_pca_pre_scale')\n",
    "#     assembler_numeric_non_pca = VectorAssembler(inputCols=numeric_non_pca_columns, outputCol='features_numeric_non_pca_pre_scale')\n",
    "\n",
    "#     # Create StandardScaler for numeric PCA and non-PCA features\n",
    "#     scaler_non_pca = StandardScaler(inputCol=\"features_numeric_non_pca_pre_scale\", outputCol=\"features_numeric_non_pca_scaled\", withStd=True, withMean=True)\n",
    "#     scaler_pca = StandardScaler(inputCol=\"features_numeric_pca_pre_scale\", outputCol=\"features_numeric_pca_scaled\", withStd=True, withMean=True)\n",
    "\n",
    "#     # Create PCA stage\n",
    "#     pca = PCA(k=2, inputCol='features_numeric_pca_scaled', outputCol='dense_vect_pca_features')\n",
    "\n",
    "#     # Create VectorAssembler for all features\n",
    "#     assembler_all = VectorAssembler(inputCols=[\"features_numeric_non_pca_scaled\", \"dense_vect_pca_features\"] + categorical_features, outputCol=\"allFeatures\")\n",
    "\n",
    "#     # Create StringIndexer for the label\n",
    "#     label = StringIndexer(inputCol=prediction_feature, outputCol=\"label\")\n",
    "\n",
    "#     # Build the list of stages for the model matrix pipeline\n",
    "#     model_matrix_stages = indexers + ohes + [imputer, assembler_numeric_non_pca, assembler_numeric_pca, scaler_non_pca, scaler_pca, pca, assembler_all, label]\n",
    "\n",
    "#     return model_matrix_stages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61a9e566-fa61-4832-9756-15183b911673",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "## Run time series cross validation and perform hyperparameter tuning to select the best model\n",
    "def tscv(dataset, model, pca_numerics, non_pca_numerics, categoricals, k=5):\n",
    "    '''\n",
    "    Description: Runs time series cross validation on the provided model\n",
    "    Input: Dataset, built model, PCA numeric features, non-PCA numeric features, categorical features, number of folds (k)\n",
    "    Output: Average F1 score of model\n",
    "    '''\n",
    "\n",
    "    # initialize variables \n",
    "    n = dataset.count()\n",
    "    chunk_size = int(n/(k+1))\n",
    "    scores_auc = []\n",
    "    scores_accuracy = []\n",
    "    scores_precision = []\n",
    "    scores_recall = []\n",
    "    scores_f1 = []\n",
    "\n",
    "    # Assume that we are ordering by FL_DATE always \n",
    "    sort_dataset = dataset.withColumn(\"row_id\", F.row_number().over(Window.partitionBy().orderBy(\"FL_DATE\")))\n",
    "\n",
    "    # Perform tscv and hyperparameter tuning \n",
    "    for i in range(k):\n",
    "        train_df = sort_dataset.filter(F.col('row_id') <= chunk_size * (i+1)).cache()\n",
    "        val_df = sort_dataset.filter((F.col('row_id') > chunk_size * (i+1)) & (F.col('row_id') <= chunk_size * (i+2))).cache()\n",
    "\n",
    "        build_pipeline_matrix = build_model_pipeline(train_df, categoricals,pca_numerics,non_pca_numerics,'model_delay')\n",
    "\n",
    "        pipeline = Pipeline (stages=build_pipeline_matrix + [model])\n",
    "\n",
    "        #Train model with train_df\n",
    "        pipeline_model = pipeline.fit(train_df)\n",
    "        \n",
    "        # impute null validation categoricals basked on mode of training categoricals\n",
    "        val_df = impute_categoricals(train_df, val_df, categoricals)\n",
    "\n",
    "        # Predict on validation set\n",
    "        predictions = pipeline_model.transform(val_df).select(\"probability\", \"label\", \"prediction\")\n",
    "        predictions = predictions.rdd.map(extract).toDF([\"p0\", \"p1\", \"label\", \"prediction\"])\n",
    "\n",
    "        valid_input_model = predictions\n",
    "\n",
    "        # Create an evaluator\n",
    "        evaluator_auc = BinaryClassificationEvaluator(labelCol='label')\n",
    "        evaluator_auc.setRawPredictionCol('p1')\n",
    "\n",
    "        # Compute the areaUnderROC on the test data\n",
    "        areaUnderROC = evaluator_auc.evaluate(valid_input_model, {evaluator_auc.metricName: \"areaUnderROC\"})\n",
    "\n",
    "        evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\")\n",
    "\n",
    "        # Compute various evaluation metrics\n",
    "        accuracy = evaluator.evaluate(valid_input_model, {evaluator.metricName: \"accuracy\"})\n",
    "        precision = evaluator.evaluate(valid_input_model, {evaluator.metricName: \"weightedPrecision\"})\n",
    "        recall = evaluator.evaluate(valid_input_model, {evaluator.metricName: \"weightedRecall\"})\n",
    "        f1 = evaluator.evaluate(valid_input_model, {evaluator.metricName: \"f1\"})\n",
    "\n",
    "        print(\"Accuracy = %g\" % accuracy)\n",
    "        print(\"Precision = %g\" % precision)\n",
    "        print(\"Recall = %g\" % recall)\n",
    "        print(\"F1 = %g\" % f1)\n",
    "\n",
    "        # Append the score to the scores list\n",
    "        scores_auc.append(areaUnderROC)\n",
    "        scores_accuracy.append(accuracy)\n",
    "        scores_precision.append(precision)\n",
    "        scores_recall.append(recall)\n",
    "        scores_f1.append(f1)\n",
    "\n",
    "    f1_score = np.mean(scores_f1)\n",
    "    return f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0cea51dc-0d93-4c5c-9beb-9939a7df6194",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_hyperparameters(dataset, model_type, hyperparameter_list, k):\n",
    "    '''\n",
    "    Description:\n",
    "    Input:\n",
    "    Output:\n",
    "    '''\n",
    "    # Initialize variables\n",
    "    best_score = 0\n",
    "    best_param = None\n",
    "    start_time = time.time()\n",
    "    iterations = 0\n",
    "    # Loop through all combinations of hyperparameters and pick the best set\n",
    "    for parameters in hyperparameter_list:\n",
    "        iterations = iterations+1\n",
    "        new_model = build_model(model_type, parameters)\n",
    "        avg_score = tscv(dataset = dataset, model = new_model, k=k)\n",
    "        if avg_score > best_score:\n",
    "            best_score = avg_score\n",
    "            best_params = parameters\n",
    "        print(parameters)\n",
    "        print('best score:', best_score, '|best params:', best_params)\n",
    "        print(\"iteration: \" + str(iterations) + \" --- %s seconds ---\" % (time.time() - start_time))\n",
    "    return best_score, best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54cb48c1-d24f-4182-a9c7-a238a9abbf6f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "## Run time series cross validation and perform hyperparameter tuning to select the best model\n",
    "def single_model_run(dataset, model, pca_numerics, non_pca_numerics, categoricals, k=5):\n",
    "    '''\n",
    "    Description: Runs time series cross validation on the provided model\n",
    "    Input: Dataset, built model, PCA numeric features, non-PCA numeric features, categorical features, number of folds (k)\n",
    "    Output: Average F1 score of model\n",
    "    '''\n",
    "    # initialize variables \n",
    "    # n = dataset.count()\n",
    "    # chunk_size = int(n/(k+1))\n",
    "    scores_auc = []\n",
    "    scores_accuracy = []\n",
    "    scores_precision = []\n",
    "    scores_recall = []\n",
    "    scores_f1 = []\n",
    "\n",
    "    # Assume that we are ordering by FL_DATE always \n",
    "    sort_dataset = dataset.withColumn(\"row_id\", F.row_number().over(Window.partitionBy().orderBy(\"FL_DATE\",F.rand())))\n",
    "\n",
    "    model_n = sort_dataset.count()\n",
    "    split_factor = 0.7\n",
    "\n",
    "    # Perform tscv and hyperparameter tuning \n",
    "\n",
    "    train_df = sort_dataset.filter(F.col('row_id') <= (model_n * split_factor)).cache()\n",
    "    val_df = sort_dataset.filter((F.col('row_id') > (model_n * split_factor))).cache()\n",
    "\n",
    "    build_pipeline_matrix = build_model_pipeline(train_df, categoricals,pca_numerics,non_pca_numerics,'model_delay')\n",
    "\n",
    "    pipeline = Pipeline (stages=build_pipeline_matrix + [model])\n",
    "\n",
    "    #Train model with train_df\n",
    "    pipeline_model = pipeline.fit(train_df)\n",
    "    \n",
    "    # impute null validation categoricals basked on mode of training categoricals\n",
    "    val_df = impute_categoricals(train_df, val_df, categoricals)\n",
    "\n",
    "    # Predict on validation set\n",
    "    predictions = pipeline_model.transform(val_df).select(\"probability\", \"label\", \"prediction\")\n",
    "    predictions = predictions.rdd.map(extract).toDF([\"p0\", \"p1\", \"label\", \"prediction\"])\n",
    "\n",
    "    valid_input_model = predictions\n",
    "\n",
    "    # Create an evaluator\n",
    "    evaluator_auc = BinaryClassificationEvaluator(labelCol='label')\n",
    "    evaluator_auc.setRawPredictionCol('p1')\n",
    "\n",
    "    # Compute the areaUnderROC on the test data\n",
    "    areaUnderROC = evaluator_auc.evaluate(valid_input_model, {evaluator_auc.metricName: \"areaUnderROC\"})\n",
    "\n",
    "    evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\")\n",
    "\n",
    "    # Compute various evaluation metrics\n",
    "    accuracy = evaluator.evaluate(valid_input_model, {evaluator.metricName: \"accuracy\"})\n",
    "    precision = evaluator.evaluate(valid_input_model, {evaluator.metricName: \"weightedPrecision\"})\n",
    "    recall = evaluator.evaluate(valid_input_model, {evaluator.metricName: \"weightedRecall\"})\n",
    "    f1 = evaluator.evaluate(valid_input_model, {evaluator.metricName: \"f1\"})\n",
    "\n",
    "    print(\"Accuracy = %g\" % accuracy)\n",
    "    print(\"Precision = %g\" % precision)\n",
    "    print(\"Recall = %g\" % recall)\n",
    "    print(\"F1 = %g\" % f1)\n",
    "\n",
    "    # Append the score to the scores list\n",
    "    scores_auc.append(areaUnderROC)\n",
    "    scores_accuracy.append(accuracy)\n",
    "    scores_precision.append(precision)\n",
    "    scores_recall.append(recall)\n",
    "    scores_f1.append(f1)\n",
    "\n",
    "    f1_score = np.mean(scores_f1)\n",
    "\n",
    "    return f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "792ca46c-f1a5-40c1-b8cd-b7bf9600c5d4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_hyperparameters_single_run(dataset, model_type, hyperparameter_list, k_folds, pca_numerics, non_pca_numerics, categoricals):\n",
    "    '''\n",
    "    Description:\n",
    "    Input:\n",
    "    Output:\n",
    "    '''\n",
    "    # Initialize variables\n",
    "    best_score = 0\n",
    "    best_param = None\n",
    "\n",
    "    # Loop through all combinations of hyperparameters and pick the best set\n",
    "    for parameters in hyperparameter_list:\n",
    "        new_model = build_model(model_type, parameters)\n",
    "        avg_score = single_model_run(dataset = dataset, model = new_model, pca_numerics = pca_num, non_pca_numerics = non_pca_num, categoricals = categoricals, k = k_folds)\n",
    "        if avg_score > best_score:\n",
    "            best_score = avg_score\n",
    "            best_params = parameters\n",
    "        print(parameters)\n",
    "        print('best score:', best_score, '|best params:', best_params)\n",
    "    return best_score, best_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f1ff42e-fb66-4beb-b07a-82dc63714db7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Run time series cross validation and perform hyperparameter tuning to select the best model\n",
    "def single_model_fit(dataset, model, pca_numerics, non_pca_numerics, categoricals, k=5):\n",
    "    '''\n",
    "    Description: Runs time series cross validation on the provided model\n",
    "    Input: Dataset, built model, PCA numeric features, non-PCA numeric features, categorical features, number of folds (k)\n",
    "    Output: Average F1 score of model\n",
    "    '''\n",
    "    # initialize variables \n",
    "    # n = dataset.count()\n",
    "    # chunk_size = int(n/(k+1))\n",
    "    scores_auc = []\n",
    "    scores_accuracy = []\n",
    "    scores_precision = []\n",
    "    scores_recall = []\n",
    "    scores_f1 = []\n",
    "\n",
    "    # Assume that we are ordering by FL_DATE always \n",
    "    sort_dataset = dataset.withColumn(\"row_id\", F.row_number().over(Window.partitionBy().orderBy(\"FL_DATE\",F.rand())))\n",
    "\n",
    "    model_n = sort_dataset.count()\n",
    "    split_factor = 0.7\n",
    "\n",
    "    # Perform tscv and hyperparameter tuning \n",
    "\n",
    "    train_df = sort_dataset.filter(F.col('row_id') <= (model_n * split_factor)).cache()\n",
    "    val_df = sort_dataset.filter((F.col('row_id') > (model_n * split_factor))).cache()\n",
    "\n",
    "    build_pipeline_matrix = build_model_pipeline(train_df, categoricals,pca_numerics,non_pca_numerics,'model_delay')\n",
    "\n",
    "    pipeline = Pipeline(stages=build_pipeline_matrix + [model])\n",
    "\n",
    "    #Train model with train_df\n",
    "    pipeline_model = pipeline.fit(train_df)\n",
    "\n",
    "    #################### Feature Importances Code ######################\n",
    "\n",
    "    ###################################################################\n",
    "    return pipeline_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b6ab9b1-0f40-4519-ac68-8bdda07f5ab6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def extract(row):\n",
    "    '''\n",
    "    Description:\n",
    "    Input:\n",
    "    Output:\n",
    "    '''\n",
    "    \n",
    "    return tuple(row.probability.toArray().tolist()) +  (row.label,) + (row.prediction,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ea78ecb-2cde-4653-bda6-eddd2eb7776b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# 4. Model Pipeline (Modelling & Hyperparameter Tuning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc40965c-818e-4f12-8a71-e24b113eaa03",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 4.1 Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56eb562d-d09f-49e9-b772-909b45589a8e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# ML Flow Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "072487ac-6711-4c16-8755-994d4678981a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def RegressionEvaluator(preds):\n",
    "    # print(preds)\n",
    "    rdd_preds_m = preds.select(['prediction', 'label']).rdd\n",
    "\n",
    "    # predictions = pipeline_model.transform(df_test_3m).select(\"probability\", \"label\", \"prediction\")\n",
    "    # predictions = predictions.rdd.map(extract).toDF([\"p0\", \"p1\", \"label\", \"prediction\"])\n",
    "\n",
    "    preds = preds.select(\"probability\", \"label\", \"prediction\")\n",
    "    preds = preds.rdd.map(extract).toDF([\"p0\", \"p1\", \"label\", \"prediction\"])\n",
    "\n",
    "\n",
    "    # Create an binary evaluator\n",
    "    evaluator_auc = BinaryClassificationEvaluator(labelCol='label')\n",
    "    evaluator_auc.setRawPredictionCol('p1')\n",
    "\n",
    "    # Compute the areaUnderROC on the test data\n",
    "    areaUnderROC = evaluator_auc.evaluate(preds, {evaluator_auc.metricName: \"areaUnderROC\"})\n",
    "    areaUnderPR = evaluator_auc.evaluate(preds, {evaluator_auc.metricName: \"areaUnderPR\"})\n",
    "\n",
    "    multi_evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\")\n",
    "    multi_evaluator2 = MulticlassMetrics(rdd_preds_m)\n",
    "    # Compute various evaluation metrics\n",
    "    accuracy = multi_evaluator.evaluate(preds, {multi_evaluator.metricName: \"accuracy\"})\n",
    "    precision = multi_evaluator.evaluate(preds, {multi_evaluator.metricName: \"precisionByLabel\"})\n",
    "    recall = multi_evaluator.evaluate(preds, {multi_evaluator.metricName: \"recallByLabel\"})\n",
    "    f1 = multi_evaluator.evaluate(preds, {multi_evaluator.metricName: \"f1\"})\n",
    "\n",
    "    f2 = np.round(multi_evaluator2.fMeasure(label=1.0, beta=2.0), 5)\n",
    "    # pr = binary_evaluator.areaUnderPR\n",
    "\n",
    "    return accuracy, precision, recall, f1, f2 ,areaUnderROC,areaUnderPR\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1df8455-cef8-4020-bdd0-11b16695fb2c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_train = df_train_12m\n",
    "df_test = df_test_12m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59bcdd7f-2dd5-4a59-a2ab-2839ef5d9947",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# cat_features = [\"REPORT_TYPE\",\"DAY_OF_WEEK\",\"flight_time_category\", \"DISTANCE_BIN\", \"CANCELLED\", \"us_holiday\"]\n",
    "\n",
    "# categoricals = list(map(lambda c: c+\"_class\", cat_features))\n",
    "\n",
    "# assemblerAll = VectorAssembler(inputCols= [\"numeric_scaled_non_pca\", \"dense_vect_pca_features\"] +categoricals , outputCol=\"features\")\n",
    "\n",
    "\n",
    "# model_matrix_stages = [assemblerAll] #+ [label]\n",
    "\n",
    "# lr = LogisticRegression()\n",
    "\n",
    "# pipeline = Pipeline(stages=model_matrix_stages + [lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b26e320e-e23f-4b72-89ea-9a14f778f789",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'maxIter': <hyperopt.pyll.base.Apply at 0x7f9b141bd720>,\n",
       " 'regParam': <hyperopt.pyll.base.Apply at 0x7f9b141bdc00>,\n",
       " 'elasticNetParam': <hyperopt.pyll.base.Apply at 0x7f9b141bc850>}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CHANGES HERE\n",
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "# Suppose `df` is your dataframe and `label` is your target column\n",
    "numNegatives = df_train_12m.filter(col(\"label\") == 0).count()\n",
    "numPositives = df_train_12m.filter(col(\"label\") == 1).count()\n",
    "total = df.count()\n",
    "\n",
    "# Calculate class weights\n",
    "weightForPositive = total / (2.0 * numPositives)\n",
    "weightForNegative = total / (2.0 * numNegatives)\n",
    "\n",
    "# Add a new column 'classWeight' to df\n",
    "df_train = df_train_12m.withColumn(\"classWeight\", when(col(\"label\") == 1, weightForPositive).otherwise(weightForNegative))\n",
    "\n",
    "numNegatives = df_test_12m.filter(col(\"label\") == 0).count()\n",
    "numPositives = df_test_12m.filter(col(\"label\") == 1).count()\n",
    "total = df.count()\n",
    "\n",
    "# Calculate class weights\n",
    "weightForPositive = total / (2.0 * numPositives)\n",
    "weightForNegative = total / (2.0 * numNegatives)\n",
    "\n",
    "# Add a new column 'classWeight' to df\n",
    "df_test = df_test_12m.withColumn(\"classWeight\", when(col(\"label\") == 1, weightForPositive).otherwise(weightForNegative))\n",
    "\n",
    "lr = LogisticRegression(featuresCol=\"allFeatures\", weightCol=\"classWeight\")\n",
    "pipeline = Pipeline(stages=[lr])\n",
    "\n",
    "\n",
    "from hyperopt import hp\n",
    "search_space = {\n",
    "    \"maxIter\": hp.quniform(\"maxIter\", 10,200,10),\n",
    "    \"regParam\": hp.quniform(\"regParam\", 0.1,0.5,0.01,),\n",
    "    \"elasticNetParam\": hp.quniform(\"elasticNetParam\", 0.0,1,0.1)\n",
    "}\n",
    "search_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be379055-fa2a-41b4-b201-c14d3c6c4e18",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def objective_function(params):\n",
    "    # CHANGES HERE\n",
    "    maxIter = params[\"maxIter\"]\n",
    "    regParam = params[\"regParam\"]\n",
    "    elasticNetParam = params[\"elasticNetParam\"]\n",
    "    start_time = time.time()\n",
    "\n",
    "    with mlflow.start_run():\n",
    "        # CHANGES HERE\n",
    "        estimator = pipeline.copy({lr.maxIter:maxIter,\n",
    "                                  lr.regParam:regParam,\n",
    "                                  lr.elasticNetParam: elasticNetParam})\n",
    "        model = estimator.fit(df_train)\n",
    "\n",
    "        preds_training = model.transform(df_train)       \n",
    "        pred_calc_training = RegressionEvaluator(preds_training)  \n",
    "\n",
    "        train_accuracy = pred_calc_training[0]\n",
    "        train_precision = pred_calc_training[1]\n",
    "        train_recall = pred_calc_training[2]\n",
    "        train_f1_score = pred_calc_training[3]\n",
    "        train_f2_score = pred_calc_training[4]\n",
    "        train_areaUnderROC = pred_calc_training[5]\n",
    "        train_areaUnderPR = pred_calc_training[6]\n",
    "\n",
    "        mlflow.log_metric('train_accuracy', train_accuracy)\n",
    "        mlflow.log_metric('train_precision', train_precision)\n",
    "        mlflow.log_metric('train_recall', train_recall)\n",
    "        mlflow.log_metric('train_f1_score', train_f1_score)\n",
    "        mlflow.log_metric('train_f2_score', train_f2_score)\n",
    "        mlflow.log_metric('train_areaUnderROC', train_areaUnderROC)\n",
    "        mlflow.log_metric('train_areaUnderPR', train_areaUnderPR)\n",
    "\n",
    "\n",
    "        print('-------------------')\n",
    "        print('Train Metrics:')\n",
    "        print('accuracy:',train_accuracy)\n",
    "        print('precision:',train_precision)\n",
    "        print('recall:',train_recall)\n",
    "\n",
    "        print('f1_score:',train_f1_score)\n",
    "        print('f2_score:',train_f2_score)\n",
    "\n",
    "        print('areaUnderROC:',str(train_areaUnderROC))\n",
    "        print('areaUnderPR:',str(train_areaUnderPR))\n",
    "\n",
    "        preds = model.transform(df_test)\n",
    "        pred_calc = RegressionEvaluator(preds)\n",
    "        val_accuracy = pred_calc[0]\n",
    "        val_precision = pred_calc[1]\n",
    "        val_recall = pred_calc[2]\n",
    "        val_f1_score = pred_calc[3]\n",
    "        val_f2_score = pred_calc[4]\n",
    "        val_areaUnderROC = pred_calc[5]\n",
    "        val_areaUnderPR = pred_calc[6]\n",
    "\n",
    "        mlflow.log_metric('val_accuracy', val_accuracy)\n",
    "        mlflow.log_metric('val_precision', val_precision)\n",
    "        mlflow.log_metric('val_recall', val_recall)\n",
    "        mlflow.log_metric('val_f1_score', val_f1_score)\n",
    "        mlflow.log_metric('val_f2_score', val_f2_score)\n",
    "        mlflow.log_metric('val_areaUnderROC', val_areaUnderROC)\n",
    "        mlflow.log_metric('val_areaUnderPR', val_areaUnderPR)\n",
    "        print('-------------------')\n",
    "        print('Validation Metrics:')\n",
    "        print('accuracy:',val_accuracy)\n",
    "        print('precision:',val_precision)\n",
    "        print('recall:',val_recall)\n",
    "        print('f1_score:',val_f1_score)\n",
    "        print('f2_score:',val_f2_score)\n",
    "        print('areaUnderROC:',val_areaUnderROC)\n",
    "        print('areaUnderPR:',val_areaUnderPR)\n",
    "\n",
    "        print('-------------------')\n",
    "        print('Model Params:')\n",
    "        print('maxIter:',maxIter)\n",
    "        print('regParam:',regParam)\n",
    "        print('elasticNetParam:',elasticNetParam)\n",
    " \n",
    "\n",
    "        mlflow.spark.log_model(model, \"XGB_Model_test_JT_3m_fast_test\")\n",
    "        print('model_logging_complete' + \" --- %s seconds ---\" % (time.time() - start_time))\n",
    "    return val_recall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0803429-0bf1-4455-8386-12316887a65e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r  0%|          | 0/10 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/08/08 20:40:00 WARNING mlflow.data.spark_dataset: Failed to infer schema for Spark dataset. Exception: Unsupported Spark Type '<class 'pyspark.ml.linalg.VectorUDT'>', MLflow schema is only supported for scalar Spark types.\n\n/databricks/spark/python/pyspark/sql/context.py:165: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n  warnings.warn(\n\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r                                                      \r-------------------\n\r  0%|          | 0/10 [02:24<?, ?trial/s, best loss=?]\r                                                      \rTrain Metrics:\n\r  0%|          | 0/10 [02:24<?, ?trial/s, best loss=?]\r                                                      \raccuracy:\n\r  0%|          | 0/10 [02:24<?, ?trial/s, best loss=?]\r                                                      \r0.8034807544741522\n\r  0%|          | 0/10 [02:24<?, ?trial/s, best loss=?]\r                                                      \rprecision:\n\r  0%|          | 0/10 [02:24<?, ?trial/s, best loss=?]\r                                                      \r0.8034807544741522\n\r  0%|          | 0/10 [02:24<?, ?trial/s, best loss=?]\r                                                      \rrecall:\n\r  0%|          | 0/10 [02:24<?, ?trial/s, best loss=?]\r                                                      \r1.0\n\r  0%|          | 0/10 [02:24<?, ?trial/s, best loss=?]\r                                                      \rf1_score:\n\r  0%|          | 0/10 [02:24<?, ?trial/s, best loss=?]\r                                                      \r0.7159281530548826\n\r  0%|          | 0/10 [02:24<?, ?trial/s, best loss=?]\r                                                      \rf2_score:\n\r  0%|          | 0/10 [02:24<?, ?trial/s, best loss=?]\r                                                      \r0.0\n\r  0%|          | 0/10 [02:24<?, ?trial/s, best loss=?]\r                                                      \rareaUnderROC:\n\r  0%|          | 0/10 [02:24<?, ?trial/s, best loss=?]\r                                                      \r0.5\n\r  0%|          | 0/10 [02:24<?, ?trial/s, best loss=?]\r                                                      \rareaUnderPR:\n\r  0%|          | 0/10 [02:24<?, ?trial/s, best loss=?]\r                                                      \r0.19651924552584774\n\r  0%|          | 0/10 [02:24<?, ?trial/s, best loss=?]\r                                                      \r-------------------\n\r  0%|          | 0/10 [02:48<?, ?trial/s, best loss=?]\r                                                      \rValidation Metrics:\n\r  0%|          | 0/10 [02:48<?, ?trial/s, best loss=?]\r                                                      \raccuracy:\n\r  0%|          | 0/10 [02:48<?, ?trial/s, best loss=?]\r                                                      \r0.8490596592692387\n\r  0%|          | 0/10 [02:48<?, ?trial/s, best loss=?]\r                                                      \rprecision:\n\r  0%|          | 0/10 [02:48<?, ?trial/s, best loss=?]\r                                                      \r0.8490596592692387\n\r  0%|          | 0/10 [02:48<?, ?trial/s, best loss=?]\r                                                      \rrecall:\n\r  0%|          | 0/10 [02:48<?, ?trial/s, best loss=?]\r                                                      \r1.0\n\r  0%|          | 0/10 [02:48<?, ?trial/s, best loss=?]\r                                                      \rf1_score:\n\r  0%|          | 0/10 [02:48<?, ?trial/s, best loss=?]\r                                                      \r0.7797501842458682\n\r  0%|          | 0/10 [02:48<?, ?trial/s, best loss=?]\r                                                      \rf2_score:\n\r  0%|          | 0/10 [02:48<?, ?trial/s, best loss=?]\r                                                      \r0.0\n\r  0%|          | 0/10 [02:48<?, ?trial/s, best loss=?]\r                                                      \rareaUnderROC:\n\r  0%|          | 0/10 [02:48<?, ?trial/s, best loss=?]\r                                                      \r0.5\n\r  0%|          | 0/10 [02:48<?, ?trial/s, best loss=?]\r                                                      \rareaUnderPR:\n\r  0%|          | 0/10 [02:48<?, ?trial/s, best loss=?]\r                                                      \r0.1509403407307613\n\r  0%|          | 0/10 [02:48<?, ?trial/s, best loss=?]\r                                                      \r-------------------\n\r  0%|          | 0/10 [02:48<?, ?trial/s, best loss=?]\r                                                      \rModel Params:\n\r  0%|          | 0/10 [02:48<?, ?trial/s, best loss=?]\r                                                      \rmaxIter:\n\r  0%|          | 0/10 [02:48<?, ?trial/s, best loss=?]\r                                                      \r130.0\n\r  0%|          | 0/10 [02:48<?, ?trial/s, best loss=?]\r                                                      \rregParam:\n\r  0%|          | 0/10 [02:48<?, ?trial/s, best loss=?]\r                                                      \r0.43\n\r  0%|          | 0/10 [02:48<?, ?trial/s, best loss=?]\r                                                      \relasticNetParam:\n\r  0%|          | 0/10 [02:48<?, ?trial/s, best loss=?]\r                                                      \r0.9\n\r  0%|          | 0/10 [02:48<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/08/08 20:42:54 INFO mlflow.spark: Inferring pip requirements by reloading the logged model from the databricks artifact repository, which can be time-consuming. To speed up, explicitly specify the conda_env or pip_requirements when calling log_model().\n\n/databricks/python/lib/python3.10/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\n  warnings.warn(\"Setuptools is replacing distutils.\")\n\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r                                                      \rmodel_logging_complete --- 217.94420051574707 seconds ---\n\r  0%|          | 0/10 [03:37<?, ?trial/s, best loss=?]\r 10%|         | 1/10 [03:38<32:43, 218.15s/trial, best loss: 1.0]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/08/08 20:43:39 WARNING mlflow.data.spark_dataset: Failed to infer schema for Spark dataset. Exception: Unsupported Spark Type '<class 'pyspark.ml.linalg.VectorUDT'>', MLflow schema is only supported for scalar Spark types.\n\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r                                                                  \r-------------------\n\r 10%|         | 1/10 [06:12<32:43, 218.15s/trial, best loss: 1.0]\r                                                                  \rTrain Metrics:\n\r 10%|         | 1/10 [06:12<32:43, 218.15s/trial, best loss: 1.0]\r                                                                  \raccuracy:\n\r 10%|         | 1/10 [06:12<32:43, 218.15s/trial, best loss: 1.0]\r                                                                  \r0.8034807544741522\n\r 10%|         | 1/10 [06:12<32:43, 218.15s/trial, best loss: 1.0]\r                                                                  \rprecision:\n\r 10%|         | 1/10 [06:12<32:43, 218.15s/trial, best loss: 1.0]\r                                                                  \r0.8034807544741522\n\r 10%|         | 1/10 [06:12<32:43, 218.15s/trial, best loss: 1.0]\r                                                                  \rrecall:\n\r 10%|         | 1/10 [06:12<32:43, 218.15s/trial, best loss: 1.0]\r                                                                  \r1.0\n\r 10%|         | 1/10 [06:12<32:43, 218.15s/trial, best loss: 1.0]\r                                                                  \rf1_score:\n\r 10%|         | 1/10 [06:12<32:43, 218.15s/trial, best loss: 1.0]\r                                                                  \r0.7159281530548826\n\r 10%|         | 1/10 [06:12<32:43, 218.15s/trial, best loss: 1.0]\r                                                                  \rf2_score:\n\r 10%|         | 1/10 [06:12<32:43, 218.15s/trial, best loss: 1.0]\r                                                                  \r0.0\n\r 10%|         | 1/10 [06:12<32:43, 218.15s/trial, best loss: 1.0]\r                                                                  \rareaUnderROC:\n\r 10%|         | 1/10 [06:12<32:43, 218.15s/trial, best loss: 1.0]\r                                                                  \r0.5\n\r 10%|         | 1/10 [06:12<32:43, 218.15s/trial, best loss: 1.0]\r                                                                  \rareaUnderPR:\n\r 10%|         | 1/10 [06:12<32:43, 218.15s/trial, best loss: 1.0]\r                                                                  \r0.19651924552584774\n\r 10%|         | 1/10 [06:12<32:43, 218.15s/trial, best loss: 1.0]\r                                                                  \r-------------------\n\r 10%|         | 1/10 [06:37<32:43, 218.15s/trial, best loss: 1.0]\r                                                                  \rValidation Metrics:\n\r 10%|         | 1/10 [06:37<32:43, 218.15s/trial, best loss: 1.0]\r                                                                  \raccuracy:\n\r 10%|         | 1/10 [06:37<32:43, 218.15s/trial, best loss: 1.0]\r                                                                  \r0.8490596592692387\n\r 10%|         | 1/10 [06:37<32:43, 218.15s/trial, best loss: 1.0]\r                                                                  \rprecision:\n\r 10%|         | 1/10 [06:37<32:43, 218.15s/trial, best loss: 1.0]\r                                                                  \r0.8490596592692387\n\r 10%|         | 1/10 [06:37<32:43, 218.15s/trial, best loss: 1.0]\r                                                                  \rrecall:\n\r 10%|         | 1/10 [06:37<32:43, 218.15s/trial, best loss: 1.0]\r                                                                  \r1.0\n\r 10%|         | 1/10 [06:37<32:43, 218.15s/trial, best loss: 1.0]\r                                                                  \rf1_score:\n\r 10%|         | 1/10 [06:37<32:43, 218.15s/trial, best loss: 1.0]\r                                                                  \r0.7797501842458682\n\r 10%|         | 1/10 [06:37<32:43, 218.15s/trial, best loss: 1.0]\r                                                                  \rf2_score:\n\r 10%|         | 1/10 [06:37<32:43, 218.15s/trial, best loss: 1.0]\r                                                                  \r0.0\n\r 10%|         | 1/10 [06:37<32:43, 218.15s/trial, best loss: 1.0]\r                                                                  \rareaUnderROC:\n\r 10%|         | 1/10 [06:37<32:43, 218.15s/trial, best loss: 1.0]\r                                                                  \r0.5\n\r 10%|         | 1/10 [06:37<32:43, 218.15s/trial, best loss: 1.0]\r                                                                  \rareaUnderPR:\n\r 10%|         | 1/10 [06:37<32:43, 218.15s/trial, best loss: 1.0]\r                                                                  \r0.1509403407307613\n\r 10%|         | 1/10 [06:37<32:43, 218.15s/trial, best loss: 1.0]\r                                                                  \r-------------------\n\r 10%|         | 1/10 [06:37<32:43, 218.15s/trial, best loss: 1.0]\r                                                                  \rModel Params:\n\r 10%|         | 1/10 [06:37<32:43, 218.15s/trial, best loss: 1.0]\r                                                                  \rmaxIter:\n\r 10%|         | 1/10 [06:37<32:43, 218.15s/trial, best loss: 1.0]\r                                                                  \r130.0\n\r 10%|         | 1/10 [06:37<32:43, 218.15s/trial, best loss: 1.0]\r                                                                  \rregParam:\n\r 10%|         | 1/10 [06:37<32:43, 218.15s/trial, best loss: 1.0]\r                                                                  \r0.22\n\r 10%|         | 1/10 [06:37<32:43, 218.15s/trial, best loss: 1.0]\r                                                                  \relasticNetParam:\n\r 10%|         | 1/10 [06:37<32:43, 218.15s/trial, best loss: 1.0]\r                                                                  \r0.9\n\r 10%|         | 1/10 [06:37<32:43, 218.15s/trial, best loss: 1.0]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/08/08 20:46:42 INFO mlflow.spark: Inferring pip requirements by reloading the logged model from the databricks artifact repository, which can be time-consuming. To speed up, explicitly specify the conda_env or pip_requirements when calling log_model().\n\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r                                                                  \rmodel_logging_complete --- 229.43493366241455 seconds ---\n\r 10%|         | 1/10 [07:27<32:43, 218.15s/trial, best loss: 1.0]\r 20%|        | 2/10 [07:27<29:59, 224.89s/trial, best loss: 1.0]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/08/08 20:47:28 WARNING mlflow.data.spark_dataset: Failed to infer schema for Spark dataset. Exception: Unsupported Spark Type '<class 'pyspark.ml.linalg.VectorUDT'>', MLflow schema is only supported for scalar Spark types.\n\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r                                                                  \r-------------------\n\r 20%|        | 2/10 [10:09<29:59, 224.89s/trial, best loss: 1.0]\r                                                                  \rTrain Metrics:\n\r 20%|        | 2/10 [10:09<29:59, 224.89s/trial, best loss: 1.0]\r                                                                  \raccuracy:\n\r 20%|        | 2/10 [10:09<29:59, 224.89s/trial, best loss: 1.0]\r                                                                  \r0.730173986652554\n\r 20%|        | 2/10 [10:09<29:59, 224.89s/trial, best loss: 1.0]\r                                                                  \rprecision:\n\r 20%|        | 2/10 [10:09<29:59, 224.89s/trial, best loss: 1.0]\r                                                                  \r0.8451982242636832\n\r 20%|        | 2/10 [10:09<29:59, 224.89s/trial, best loss: 1.0]\r                                                                  \rrecall:\n\r 20%|        | 2/10 [10:09<29:59, 224.89s/trial, best loss: 1.0]\r                                                                  \r0.8131017967497374\n\r 20%|        | 2/10 [10:09<29:59, 224.89s/trial, best loss: 1.0]\r                                                                  \rf1_score:\n\r 20%|        | 2/10 [10:09<29:59, 224.89s/trial, best loss: 1.0]\r                                                                  \r0.7372817527113507\n\r 20%|        | 2/10 [10:09<29:59, 224.89s/trial, best loss: 1.0]\r                                                                  \rf2_score:\n\r 20%|        | 2/10 [10:09<29:59, 224.89s/trial, best loss: 1.0]\r                                                                  \r0.37934\n\r 20%|        | 2/10 [10:09<29:59, 224.89s/trial, best loss: 1.0]\r                                                                  \rareaUnderROC:\n\r 20%|        | 2/10 [10:09<29:59, 224.89s/trial, best loss: 1.0]\r                                                                  \r0.6432970619979722\n\r 20%|        | 2/10 [10:09<29:59, 224.89s/trial, best loss: 1.0]\r                                                                  \rareaUnderPR:\n\r 20%|        | 2/10 [10:09<29:59, 224.89s/trial, best loss: 1.0]\r                                                                  \r0.2963860528261424\n\r 20%|        | 2/10 [10:09<29:59, 224.89s/trial, best loss: 1.0]\r                                                                  \r-------------------\n\r 20%|        | 2/10 [10:29<29:59, 224.89s/trial, best loss: 1.0]\r                                                                  \rValidation Metrics:\n\r 20%|        | 2/10 [10:29<29:59, 224.89s/trial, best loss: 1.0]\r                                                                  \raccuracy:\n\r 20%|        | 2/10 [10:29<29:59, 224.89s/trial, best loss: 1.0]\r                                                                  \r0.7382290588047258\n\r 20%|        | 2/10 [10:29<29:59, 224.89s/trial, best loss: 1.0]\r                                                                  \rprecision:\n\r 20%|        | 2/10 [10:29<29:59, 224.89s/trial, best loss: 1.0]\r                                                                  \r0.8764188159369899\n\r 20%|        | 2/10 [10:29<29:59, 224.89s/trial, best loss: 1.0]\r                                                                  \rrecall:\n\r 20%|        | 2/10 [10:29<29:59, 224.89s/trial, best loss: 1.0]\r                                                                  \r0.8052371887743818\n\r 20%|        | 2/10 [10:29<29:59, 224.89s/trial, best loss: 1.0]\r                                                                  \rf1_score:\n\r 20%|        | 2/10 [10:29<29:59, 224.89s/trial, best loss: 1.0]\r                                                                  \r0.7570276923557908\n\r 20%|        | 2/10 [10:29<29:59, 224.89s/trial, best loss: 1.0]\r                                                                  \rf2_score:\n\r 20%|        | 2/10 [10:29<29:59, 224.89s/trial, best loss: 1.0]\r                                                                  \r0.33105\n\r 20%|        | 2/10 [10:29<29:59, 224.89s/trial, best loss: 1.0]\r                                                                  \rareaUnderROC:\n\r 20%|        | 2/10 [10:29<29:59, 224.89s/trial, best loss: 1.0]\r                                                                  \r0.6246518773015473\n\r 20%|        | 2/10 [10:29<29:59, 224.89s/trial, best loss: 1.0]\r                                                                  \rareaUnderPR:\n\r 20%|        | 2/10 [10:29<29:59, 224.89s/trial, best loss: 1.0]\r                                                                  \r0.2187062673557174\n\r 20%|        | 2/10 [10:29<29:59, 224.89s/trial, best loss: 1.0]\r                                                                  \r-------------------\n\r 20%|        | 2/10 [10:29<29:59, 224.89s/trial, best loss: 1.0]\r                                                                  \rModel Params:\n\r 20%|        | 2/10 [10:29<29:59, 224.89s/trial, best loss: 1.0]\r                                                                  \rmaxIter:\n\r 20%|        | 2/10 [10:29<29:59, 224.89s/trial, best loss: 1.0]\r                                                                  \r20.0\n\r 20%|        | 2/10 [10:29<29:59, 224.89s/trial, best loss: 1.0]\r                                                                  \rregParam:\n\r 20%|        | 2/10 [10:29<29:59, 224.89s/trial, best loss: 1.0]\r                                                                  \r0.16\n\r 20%|        | 2/10 [10:29<29:59, 224.89s/trial, best loss: 1.0]\r                                                                  \relasticNetParam:\n\r 20%|        | 2/10 [10:29<29:59, 224.89s/trial, best loss: 1.0]\r                                                                  \r0.6000000000000001\n\r 20%|        | 2/10 [10:29<29:59, 224.89s/trial, best loss: 1.0]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/08/08 20:50:33 INFO mlflow.spark: Inferring pip requirements by reloading the logged model from the databricks artifact repository, which can be time-consuming. To speed up, explicitly specify the conda_env or pip_requirements when calling log_model().\n\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r                                                                  \rmodel_logging_complete --- 227.6475853919983 seconds ---\n\r 20%|        | 2/10 [11:15<29:59, 224.89s/trial, best loss: 1.0]\r 30%|       | 3/10 [11:15<26:23, 226.25s/trial, best loss: 0.8052371887743818]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/08/08 20:51:16 WARNING mlflow.data.spark_dataset: Failed to infer schema for Spark dataset. Exception: Unsupported Spark Type '<class 'pyspark.ml.linalg.VectorUDT'>', MLflow schema is only supported for scalar Spark types.\n\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r                                                                                 \r-------------------\n\r 30%|       | 3/10 [14:10<26:23, 226.25s/trial, best loss: 0.8052371887743818]\r                                                                                 \rTrain Metrics:\n\r 30%|       | 3/10 [14:10<26:23, 226.25s/trial, best loss: 0.8052371887743818]\r                                                                                 \raccuracy:\n\r 30%|       | 3/10 [14:10<26:23, 226.25s/trial, best loss: 0.8052371887743818]\r                                                                                 \r0.8034807544741522\n\r 30%|       | 3/10 [14:10<26:23, 226.25s/trial, best loss: 0.8052371887743818]\r                                                                                 \rprecision:\n\r 30%|       | 3/10 [14:10<26:23, 226.25s/trial, best loss: 0.8052371887743818]\r                                                                                 \r0.8034807544741522\n\r 30%|       | 3/10 [14:10<26:23, 226.25s/trial, best loss: 0.8052371887743818]\r                                                                                 \rrecall:\n\r 30%|       | 3/10 [14:10<26:23, 226.25s/trial, best loss: 0.8052371887743818]\r                                                                                 \r1.0\n\r 30%|       | 3/10 [14:10<26:23, 226.25s/trial, best loss: 0.8052371887743818]\r                                                                                 \rf1_score:\n\r 30%|       | 3/10 [14:10<26:23, 226.25s/trial, best loss: 0.8052371887743818]\r                                                                                 \r0.7159281530548826\n\r 30%|       | 3/10 [14:10<26:23, 226.25s/trial, best loss: 0.8052371887743818]\r                                                                                 \rf2_score:\n\r 30%|       | 3/10 [14:10<26:23, 226.25s/trial, best loss: 0.8052371887743818]\r                                                                                 \r0.0\n\r 30%|       | 3/10 [14:10<26:23, 226.25s/trial, best loss: 0.8052371887743818]\r                                                                                 \rareaUnderROC:\n\r 30%|       | 3/10 [14:10<26:23, 226.25s/trial, best loss: 0.8052371887743818]\r                                                                                 \r0.5\n\r 30%|       | 3/10 [14:10<26:23, 226.25s/trial, best loss: 0.8052371887743818]\r                                                                                 \rareaUnderPR:\n\r 30%|       | 3/10 [14:10<26:23, 226.25s/trial, best loss: 0.8052371887743818]\r                                                                                 \r0.19651924552584774\n\r 30%|       | 3/10 [14:10<26:23, 226.25s/trial, best loss: 0.8052371887743818]\r                                                                                 \r-------------------\n\r 30%|       | 3/10 [14:30<26:23, 226.25s/trial, best loss: 0.8052371887743818]\r                                                                                 \rValidation Metrics:\n\r 30%|       | 3/10 [14:30<26:23, 226.25s/trial, best loss: 0.8052371887743818]\r                                                                                 \raccuracy:\n\r 30%|       | 3/10 [14:30<26:23, 226.25s/trial, best loss: 0.8052371887743818]\r                                                                                 \r0.8490596592692387\n\r 30%|       | 3/10 [14:30<26:23, 226.25s/trial, best loss: 0.8052371887743818]\r                                                                                 \rprecision:\n\r 30%|       | 3/10 [14:30<26:23, 226.25s/trial, best loss: 0.8052371887743818]\r                                                                                 \r0.8490596592692387\n\r 30%|       | 3/10 [14:30<26:23, 226.25s/trial, best loss: 0.8052371887743818]\r                                                                                 \rrecall:\n\r 30%|       | 3/10 [14:30<26:23, 226.25s/trial, best loss: 0.8052371887743818]\r                                                                                 \r1.0\n\r 30%|       | 3/10 [14:30<26:23, 226.25s/trial, best loss: 0.8052371887743818]\r                                                                                 \rf1_score:\n\r 30%|       | 3/10 [14:30<26:23, 226.25s/trial, best loss: 0.8052371887743818]\r                                                                                 \r0.7797501842458682\n\r 30%|       | 3/10 [14:30<26:23, 226.25s/trial, best loss: 0.8052371887743818]\r                                                                                 \rf2_score:\n\r 30%|       | 3/10 [14:30<26:23, 226.25s/trial, best loss: 0.8052371887743818]\r                                                                                 \r0.0\n\r 30%|       | 3/10 [14:30<26:23, 226.25s/trial, best loss: 0.8052371887743818]\r                                                                                 \rareaUnderROC:\n\r 30%|       | 3/10 [14:30<26:23, 226.25s/trial, best loss: 0.8052371887743818]\r                                                                                 \r0.5\n\r 30%|       | 3/10 [14:30<26:23, 226.25s/trial, best loss: 0.8052371887743818]\r                                                                                 \rareaUnderPR:\n\r 30%|       | 3/10 [14:30<26:23, 226.25s/trial, best loss: 0.8052371887743818]\r                                                                                 \r0.1509403407307613\n\r 30%|       | 3/10 [14:30<26:23, 226.25s/trial, best loss: 0.8052371887743818]\r                                                                                 \r-------------------\n\r 30%|       | 3/10 [14:30<26:23, 226.25s/trial, best loss: 0.8052371887743818]\r                                                                                 \rModel Params:\n\r 30%|       | 3/10 [14:30<26:23, 226.25s/trial, best loss: 0.8052371887743818]\r                                                                                 \rmaxIter:\n\r 30%|       | 3/10 [14:30<26:23, 226.25s/trial, best loss: 0.8052371887743818]\r                                                                                 \r50.0\n\r 30%|       | 3/10 [14:30<26:23, 226.25s/trial, best loss: 0.8052371887743818]\r                                                                                 \rregParam:\n\r 30%|       | 3/10 [14:30<26:23, 226.25s/trial, best loss: 0.8052371887743818]\r                                                                                 \r0.26\n\r 30%|       | 3/10 [14:30<26:23, 226.25s/trial, best loss: 0.8052371887743818]\r                                                                                 \relasticNetParam:\n\r 30%|       | 3/10 [14:30<26:23, 226.25s/trial, best loss: 0.8052371887743818]\r                                                                                 \r0.6000000000000001\n\r 30%|       | 3/10 [14:30<26:23, 226.25s/trial, best loss: 0.8052371887743818]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/08/08 20:54:36 INFO mlflow.spark: Inferring pip requirements by reloading the logged model from the databricks artifact repository, which can be time-consuming. To speed up, explicitly specify the conda_env or pip_requirements when calling log_model().\n\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r                                                                                 \rmodel_logging_complete --- 242.6640818119049 seconds ---\n\r 30%|       | 3/10 [15:18<26:23, 226.25s/trial, best loss: 0.8052371887743818]\r 40%|      | 4/10 [15:18<23:16, 232.81s/trial, best loss: 0.8052371887743818]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/08/08 20:55:19 WARNING mlflow.data.spark_dataset: Failed to infer schema for Spark dataset. Exception: Unsupported Spark Type '<class 'pyspark.ml.linalg.VectorUDT'>', MLflow schema is only supported for scalar Spark types.\n\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r                                                                                 \r-------------------\n\r 40%|      | 4/10 [21:02<23:16, 232.81s/trial, best loss: 0.8052371887743818]\r                                                                                 \rTrain Metrics:\n\r 40%|      | 4/10 [21:02<23:16, 232.81s/trial, best loss: 0.8052371887743818]\r                                                                                 \raccuracy:\n\r 40%|      | 4/10 [21:02<23:16, 232.81s/trial, best loss: 0.8052371887743818]\r                                                                                 \r0.730173986652554\n\r 40%|      | 4/10 [21:02<23:16, 232.81s/trial, best loss: 0.8052371887743818]\r                                                                                 \rprecision:\n\r 40%|      | 4/10 [21:02<23:16, 232.81s/trial, best loss: 0.8052371887743818]\r                                                                                 \r0.8451982242636832\n\r 40%|      | 4/10 [21:02<23:16, 232.81s/trial, best loss: 0.8052371887743818]\r                                                                                 \rrecall:\n\r 40%|      | 4/10 [21:02<23:16, 232.81s/trial, best loss: 0.8052371887743818]\r                                                                                 \r0.8131017967497374\n\r 40%|      | 4/10 [21:02<23:16, 232.81s/trial, best loss: 0.8052371887743818]\r                                                                                 \rf1_score:\n\r 40%|      | 4/10 [21:02<23:16, 232.81s/trial, best loss: 0.8052371887743818]\r                                                                                 \r0.7372817527113507\n\r 40%|      | 4/10 [21:02<23:16, 232.81s/trial, best loss: 0.8052371887743818]\r                                                                                 \rf2_score:\n\r 40%|      | 4/10 [21:02<23:16, 232.81s/trial, best loss: 0.8052371887743818]\r                                                                                 \r0.37934\n\r 40%|      | 4/10 [21:02<23:16, 232.81s/trial, best loss: 0.8052371887743818]\r                                                                                 \rareaUnderROC:\n\r 40%|      | 4/10 [21:02<23:16, 232.81s/trial, best loss: 0.8052371887743818]\r                                                                                 \r0.6432970619979722\n\r 40%|      | 4/10 [21:02<23:16, 232.81s/trial, best loss: 0.8052371887743818]\r                                                                                 \rareaUnderPR:\n\r 40%|      | 4/10 [21:02<23:16, 232.81s/trial, best loss: 0.8052371887743818]\r                                                                                 \r0.2963860528261424\n\r 40%|      | 4/10 [21:02<23:16, 232.81s/trial, best loss: 0.8052371887743818]\r                                                                                 \r-------------------\n\r 40%|      | 4/10 [21:34<23:16, 232.81s/trial, best loss: 0.8052371887743818]\r                                                                                 \rValidation Metrics:\n\r 40%|      | 4/10 [21:34<23:16, 232.81s/trial, best loss: 0.8052371887743818]\r                                                                                 \raccuracy:\n\r 40%|      | 4/10 [21:34<23:16, 232.81s/trial, best loss: 0.8052371887743818]\r                                                                                 \r0.7382290588047258\n\r 40%|      | 4/10 [21:34<23:16, 232.81s/trial, best loss: 0.8052371887743818]\r                                                                                 \rprecision:\n\r 40%|      | 4/10 [21:34<23:16, 232.81s/trial, best loss: 0.8052371887743818]\r                                                                                 \r0.8764188159369899\n\r 40%|      | 4/10 [21:34<23:16, 232.81s/trial, best loss: 0.8052371887743818]\r                                                                                 \rrecall:\n\r 40%|      | 4/10 [21:34<23:16, 232.81s/trial, best loss: 0.8052371887743818]\r                                                                                 \r0.8052371887743818\n\r 40%|      | 4/10 [21:34<23:16, 232.81s/trial, best loss: 0.8052371887743818]\r                                                                                 \rf1_score:\n\r 40%|      | 4/10 [21:34<23:16, 232.81s/trial, best loss: 0.8052371887743818]\r                                                                                 \r0.7570276923557908\n\r 40%|      | 4/10 [21:34<23:16, 232.81s/trial, best loss: 0.8052371887743818]\r                                                                                 \rf2_score:\n\r 40%|      | 4/10 [21:34<23:16, 232.81s/trial, best loss: 0.8052371887743818]\r                                                                                 \r0.33105\n\r 40%|      | 4/10 [21:34<23:16, 232.81s/trial, best loss: 0.8052371887743818]\r                                                                                 \rareaUnderROC:\n\r 40%|      | 4/10 [21:34<23:16, 232.81s/trial, best loss: 0.8052371887743818]\r                                                                                 \r0.6246518773015473\n\r 40%|      | 4/10 [21:34<23:16, 232.81s/trial, best loss: 0.8052371887743818]\r                                                                                 \rareaUnderPR:\n\r 40%|      | 4/10 [21:34<23:16, 232.81s/trial, best loss: 0.8052371887743818]\r                                                                                 \r0.2187062673557174\n\r 40%|      | 4/10 [21:34<23:16, 232.81s/trial, best loss: 0.8052371887743818]\r                                                                                 \r-------------------\n\r 40%|      | 4/10 [21:34<23:16, 232.81s/trial, best loss: 0.8052371887743818]\r                                                                                 \rModel Params:\n\r 40%|      | 4/10 [21:34<23:16, 232.81s/trial, best loss: 0.8052371887743818]\r                                                                                 \rmaxIter:\n\r 40%|      | 4/10 [21:34<23:16, 232.81s/trial, best loss: 0.8052371887743818]\r                                                                                 \r30.0\n\r 40%|      | 4/10 [21:34<23:16, 232.81s/trial, best loss: 0.8052371887743818]\r                                                                                 \rregParam:\n\r 40%|      | 4/10 [21:34<23:16, 232.81s/trial, best loss: 0.8052371887743818]\r                                                                                 \r0.3\n\r 40%|      | 4/10 [21:34<23:16, 232.81s/trial, best loss: 0.8052371887743818]\r                                                                                 \relasticNetParam:\n\r 40%|      | 4/10 [21:34<23:16, 232.81s/trial, best loss: 0.8052371887743818]\r                                                                                 \r0.30000000000000004\n\r 40%|      | 4/10 [21:34<23:16, 232.81s/trial, best loss: 0.8052371887743818]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/08/08 21:01:42 INFO mlflow.spark: Inferring pip requirements by reloading the logged model from the databricks artifact repository, which can be time-consuming. To speed up, explicitly specify the conda_env or pip_requirements when calling log_model().\n\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r                                                                                 \rmodel_logging_complete --- 425.1838915348053 seconds ---\n\r 40%|      | 4/10 [22:23<23:16, 232.81s/trial, best loss: 0.8052371887743818]\r 50%|     | 5/10 [22:23<25:11, 302.24s/trial, best loss: 0.8052371887743818]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/08/08 21:02:24 WARNING mlflow.data.spark_dataset: Failed to infer schema for Spark dataset. Exception: Unsupported Spark Type '<class 'pyspark.ml.linalg.VectorUDT'>', MLflow schema is only supported for scalar Spark types.\n\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r                                                                                 \r-------------------\n\r 50%|     | 5/10 [24:59<25:11, 302.24s/trial, best loss: 0.8052371887743818]\r                                                                                 \rTrain Metrics:\n\r 50%|     | 5/10 [24:59<25:11, 302.24s/trial, best loss: 0.8052371887743818]\r                                                                                 \raccuracy:\n\r 50%|     | 5/10 [24:59<25:11, 302.24s/trial, best loss: 0.8052371887743818]\r                                                                                 \r0.8034807544741522\n\r 50%|     | 5/10 [24:59<25:11, 302.24s/trial, best loss: 0.8052371887743818]\r                                                                                 \rprecision:\n\r 50%|     | 5/10 [24:59<25:11, 302.24s/trial, best loss: 0.8052371887743818]\r                                                                                 \r0.8034807544741522\n\r 50%|     | 5/10 [24:59<25:11, 302.24s/trial, best loss: 0.8052371887743818]\r                                                                                 \rrecall:\n\r 50%|     | 5/10 [24:59<25:11, 302.24s/trial, best loss: 0.8052371887743818]\r                                                                                 \r1.0\n\r 50%|     | 5/10 [24:59<25:11, 302.24s/trial, best loss: 0.8052371887743818]\r                                                                                 \rf1_score:\n\r 50%|     | 5/10 [24:59<25:11, 302.24s/trial, best loss: 0.8052371887743818]\r                                                                                 \r0.7159281530548826\n\r 50%|     | 5/10 [24:59<25:11, 302.24s/trial, best loss: 0.8052371887743818]\r                                                                                 \rf2_score:\n\r 50%|     | 5/10 [24:59<25:11, 302.24s/trial, best loss: 0.8052371887743818]\r                                                                                 \r0.0\n\r 50%|     | 5/10 [24:59<25:11, 302.24s/trial, best loss: 0.8052371887743818]\r                                                                                 \rareaUnderROC:\n\r 50%|     | 5/10 [24:59<25:11, 302.24s/trial, best loss: 0.8052371887743818]\r                                                                                 \r0.5\n\r 50%|     | 5/10 [24:59<25:11, 302.24s/trial, best loss: 0.8052371887743818]\r                                                                                 \rareaUnderPR:\n\r 50%|     | 5/10 [24:59<25:11, 302.24s/trial, best loss: 0.8052371887743818]\r                                                                                 \r0.19651924552584774\n\r 50%|     | 5/10 [24:59<25:11, 302.24s/trial, best loss: 0.8052371887743818]\r                                                                                 \r-------------------\n\r 50%|     | 5/10 [25:26<25:11, 302.24s/trial, best loss: 0.8052371887743818]\r                                                                                 \rValidation Metrics:\n\r 50%|     | 5/10 [25:26<25:11, 302.24s/trial, best loss: 0.8052371887743818]\r                                                                                 \raccuracy:\n\r 50%|     | 5/10 [25:26<25:11, 302.24s/trial, best loss: 0.8052371887743818]\r                                                                                 \r0.8490596592692387\n\r 50%|     | 5/10 [25:26<25:11, 302.24s/trial, best loss: 0.8052371887743818]\r                                                                                 \rprecision:\n\r 50%|     | 5/10 [25:26<25:11, 302.24s/trial, best loss: 0.8052371887743818]\r                                                                                 \r0.8490596592692387\n\r 50%|     | 5/10 [25:26<25:11, 302.24s/trial, best loss: 0.8052371887743818]\r                                                                                 \rrecall:\n\r 50%|     | 5/10 [25:26<25:11, 302.24s/trial, best loss: 0.8052371887743818]\r                                                                                 \r1.0\n\r 50%|     | 5/10 [25:26<25:11, 302.24s/trial, best loss: 0.8052371887743818]\r                                                                                 \rf1_score:\n\r 50%|     | 5/10 [25:26<25:11, 302.24s/trial, best loss: 0.8052371887743818]\r                                                                                 \r0.7797501842458682\n\r 50%|     | 5/10 [25:26<25:11, 302.24s/trial, best loss: 0.8052371887743818]\r                                                                                 \rf2_score:\n\r 50%|     | 5/10 [25:26<25:11, 302.24s/trial, best loss: 0.8052371887743818]\r                                                                                 \r0.0\n\r 50%|     | 5/10 [25:26<25:11, 302.24s/trial, best loss: 0.8052371887743818]\r                                                                                 \rareaUnderROC:\n\r 50%|     | 5/10 [25:26<25:11, 302.24s/trial, best loss: 0.8052371887743818]\r                                                                                 \r0.5\n\r 50%|     | 5/10 [25:26<25:11, 302.24s/trial, best loss: 0.8052371887743818]\r                                                                                 \rareaUnderPR:\n\r 50%|     | 5/10 [25:26<25:11, 302.24s/trial, best loss: 0.8052371887743818]\r                                                                                 \r0.1509403407307613\n\r 50%|     | 5/10 [25:26<25:11, 302.24s/trial, best loss: 0.8052371887743818]\r                                                                                 \r-------------------\n\r 50%|     | 5/10 [25:26<25:11, 302.24s/trial, best loss: 0.8052371887743818]\r                                                                                 \rModel Params:\n\r 50%|     | 5/10 [25:26<25:11, 302.24s/trial, best loss: 0.8052371887743818]\r                                                                                 \rmaxIter:\n\r 50%|     | 5/10 [25:26<25:11, 302.24s/trial, best loss: 0.8052371887743818]\r                                                                                 \r40.0\n\r 50%|     | 5/10 [25:26<25:11, 302.24s/trial, best loss: 0.8052371887743818]\r                                                                                 \rregParam:\n\r 50%|     | 5/10 [25:26<25:11, 302.24s/trial, best loss: 0.8052371887743818]\r                                                                                 \r0.4\n\r 50%|     | 5/10 [25:26<25:11, 302.24s/trial, best loss: 0.8052371887743818]\r                                                                                 \relasticNetParam:\n\r 50%|     | 5/10 [25:26<25:11, 302.24s/trial, best loss: 0.8052371887743818]\r                                                                                 \r0.4\n\r 50%|     | 5/10 [25:26<25:11, 302.24s/trial, best loss: 0.8052371887743818]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/08/08 21:05:33 INFO mlflow.spark: Inferring pip requirements by reloading the logged model from the databricks artifact repository, which can be time-consuming. To speed up, explicitly specify the conda_env or pip_requirements when calling log_model().\n\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r                                                                                 \rmodel_logging_complete --- 233.10661363601685 seconds ---\n\r 50%|     | 5/10 [26:16<25:11, 302.24s/trial, best loss: 0.8052371887743818]\r 60%|    | 6/10 [26:17<18:35, 278.79s/trial, best loss: 0.8052371887743818]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/08/08 21:06:18 WARNING mlflow.data.spark_dataset: Failed to infer schema for Spark dataset. Exception: Unsupported Spark Type '<class 'pyspark.ml.linalg.VectorUDT'>', MLflow schema is only supported for scalar Spark types.\n\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r                                                                                 \r-------------------\n\r 60%|    | 6/10 [28:54<18:35, 278.79s/trial, best loss: 0.8052371887743818]\r                                                                                 \rTrain Metrics:\n\r 60%|    | 6/10 [28:54<18:35, 278.79s/trial, best loss: 0.8052371887743818]\r                                                                                 \raccuracy:\n\r 60%|    | 6/10 [28:54<18:35, 278.79s/trial, best loss: 0.8052371887743818]\r                                                                                 \r0.6499266490556306\n\r 60%|    | 6/10 [28:54<18:35, 278.79s/trial, best loss: 0.8052371887743818]\r                                                                                 \rprecision:\n\r 60%|    | 6/10 [28:54<18:35, 278.79s/trial, best loss: 0.8052371887743818]\r                                                                                 \r0.8661708473239655\n\r 60%|    | 6/10 [28:54<18:35, 278.79s/trial, best loss: 0.8052371887743818]\r                                                                                 \rrecall:\n\r 60%|    | 6/10 [28:54<18:35, 278.79s/trial, best loss: 0.8052371887743818]\r                                                                                 \r0.6674257074890791\n\r 60%|    | 6/10 [28:54<18:35, 278.79s/trial, best loss: 0.8052371887743818]\r                                                                                 \rf1_score:\n\r 60%|    | 6/10 [28:54<18:35, 278.79s/trial, best loss: 0.8052371887743818]\r                                                                                 \r0.6831312705103282\n\r 60%|    | 6/10 [28:54<18:35, 278.79s/trial, best loss: 0.8052371887743818]\r                                                                                 \rf2_score:\n\r 60%|    | 6/10 [28:54<18:35, 278.79s/trial, best loss: 0.8052371887743818]\r                                                                                 \r0.48701\n\r 60%|    | 6/10 [28:54<18:35, 278.79s/trial, best loss: 0.8052371887743818]\r                                                                                 \rareaUnderROC:\n\r 60%|    | 6/10 [28:54<18:35, 278.79s/trial, best loss: 0.8052371887743818]\r                                                                                 \r0.6697300784634367\n\r 60%|    | 6/10 [28:54<18:35, 278.79s/trial, best loss: 0.8052371887743818]\r                                                                                 \rareaUnderPR:\n\r 60%|    | 6/10 [28:54<18:35, 278.79s/trial, best loss: 0.8052371887743818]\r                                                                                 \r0.3254381792559489\n\r 60%|    | 6/10 [28:54<18:35, 278.79s/trial, best loss: 0.8052371887743818]\r                                                                                 \r-------------------\n\r 60%|    | 6/10 [29:19<18:35, 278.79s/trial, best loss: 0.8052371887743818]\r                                                                                 \rValidation Metrics:\n\r 60%|    | 6/10 [29:19<18:35, 278.79s/trial, best loss: 0.8052371887743818]\r                                                                                 \raccuracy:\n\r 60%|    | 6/10 [29:19<18:35, 278.79s/trial, best loss: 0.8052371887743818]\r                                                                                 \r0.5378333313112653\n\r 60%|    | 6/10 [29:19<18:35, 278.79s/trial, best loss: 0.8052371887743818]\r                                                                                 \rprecision:\n\r 60%|    | 6/10 [29:19<18:35, 278.79s/trial, best loss: 0.8052371887743818]\r                                                                                 \r0.9072724122766102\n\r 60%|    | 6/10 [29:19<18:35, 278.79s/trial, best loss: 0.8052371887743818]\r                                                                                 \rrecall:\n\r 60%|    | 6/10 [29:19<18:35, 278.79s/trial, best loss: 0.8052371887743818]\r                                                                                 \r0.5075459554026953\n\r 60%|    | 6/10 [29:19<18:35, 278.79s/trial, best loss: 0.8052371887743818]\r                                                                                 \rf1_score:\n\r 60%|    | 6/10 [29:19<18:35, 278.79s/trial, best loss: 0.8052371887743818]\r                                                                                 \r0.6004282340178176\n\r 60%|    | 6/10 [29:19<18:35, 278.79s/trial, best loss: 0.8052371887743818]\r                                                                                 \rf2_score:\n\r 60%|    | 6/10 [29:19<18:35, 278.79s/trial, best loss: 0.8052371887743818]\r                                                                                 \r0.4735\n\r 60%|    | 6/10 [29:19<18:35, 278.79s/trial, best loss: 0.8052371887743818]\r                                                                                 \rareaUnderROC:\n\r 60%|    | 6/10 [29:19<18:35, 278.79s/trial, best loss: 0.8052371887743818]\r                                                                                 \r0.6467431236448048\n\r 60%|    | 6/10 [29:19<18:35, 278.79s/trial, best loss: 0.8052371887743818]\r                                                                                 \rareaUnderPR:\n\r 60%|    | 6/10 [29:19<18:35, 278.79s/trial, best loss: 0.8052371887743818]\r                                                                                 \r0.23563071418420775\n\r 60%|    | 6/10 [29:19<18:35, 278.79s/trial, best loss: 0.8052371887743818]\r                                                                                 \r-------------------\n\r 60%|    | 6/10 [29:19<18:35, 278.79s/trial, best loss: 0.8052371887743818]\r                                                                                 \rModel Params:\n\r 60%|    | 6/10 [29:19<18:35, 278.79s/trial, best loss: 0.8052371887743818]\r                                                                                 \rmaxIter:\n\r 60%|    | 6/10 [29:19<18:35, 278.79s/trial, best loss: 0.8052371887743818]\r                                                                                 \r200.0\n\r 60%|    | 6/10 [29:19<18:35, 278.79s/trial, best loss: 0.8052371887743818]\r                                                                                 \rregParam:\n\r 60%|    | 6/10 [29:19<18:35, 278.79s/trial, best loss: 0.8052371887743818]\r                                                                                 \r0.38\n\r 60%|    | 6/10 [29:19<18:35, 278.79s/trial, best loss: 0.8052371887743818]\r                                                                                 \relasticNetParam:\n\r 60%|    | 6/10 [29:19<18:35, 278.79s/trial, best loss: 0.8052371887743818]\r                                                                                 \r0.1\n\r 60%|    | 6/10 [29:19<18:35, 278.79s/trial, best loss: 0.8052371887743818]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/08/08 21:09:25 INFO mlflow.spark: Inferring pip requirements by reloading the logged model from the databricks artifact repository, which can be time-consuming. To speed up, explicitly specify the conda_env or pip_requirements when calling log_model().\n\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r                                                                                 \rmodel_logging_complete --- 233.14602327346802 seconds ---\n\r 60%|    | 6/10 [30:10<18:35, 278.79s/trial, best loss: 0.8052371887743818]\r 70%|   | 7/10 [30:10<13:11, 263.94s/trial, best loss: 0.5075459554026953]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/08/08 21:10:11 WARNING mlflow.data.spark_dataset: Failed to infer schema for Spark dataset. Exception: Unsupported Spark Type '<class 'pyspark.ml.linalg.VectorUDT'>', MLflow schema is only supported for scalar Spark types.\n\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r                                                                                 \r-------------------\n\r 70%|   | 7/10 [33:48<13:11, 263.94s/trial, best loss: 0.5075459554026953]\r                                                                                 \rTrain Metrics:\n\r 70%|   | 7/10 [33:48<13:11, 263.94s/trial, best loss: 0.5075459554026953]\r                                                                                 \raccuracy:\n\r 70%|   | 7/10 [33:48<13:11, 263.94s/trial, best loss: 0.5075459554026953]\r                                                                                 \r0.730173986652554\n\r 70%|   | 7/10 [33:48<13:11, 263.94s/trial, best loss: 0.5075459554026953]\r                                                                                 \rprecision:\n\r 70%|   | 7/10 [33:48<13:11, 263.94s/trial, best loss: 0.5075459554026953]\r                                                                                 \r0.8451982242636832\n\r 70%|   | 7/10 [33:48<13:11, 263.94s/trial, best loss: 0.5075459554026953]\r                                                                                 \rrecall:\n\r 70%|   | 7/10 [33:48<13:11, 263.94s/trial, best loss: 0.5075459554026953]\r                                                                                 \r0.8131017967497374\n\r 70%|   | 7/10 [33:48<13:11, 263.94s/trial, best loss: 0.5075459554026953]\r                                                                                 \rf1_score:\n\r 70%|   | 7/10 [33:48<13:11, 263.94s/trial, best loss: 0.5075459554026953]\r                                                                                 \r0.7372817527113507\n\r 70%|   | 7/10 [33:48<13:11, 263.94s/trial, best loss: 0.5075459554026953]\r                                                                                 \rf2_score:\n\r 70%|   | 7/10 [33:48<13:11, 263.94s/trial, best loss: 0.5075459554026953]\r                                                                                 \r0.37934\n\r 70%|   | 7/10 [33:48<13:11, 263.94s/trial, best loss: 0.5075459554026953]\r                                                                                 \rareaUnderROC:\n\r 70%|   | 7/10 [33:48<13:11, 263.94s/trial, best loss: 0.5075459554026953]\r                                                                                 \r0.6432970619979722\n\r 70%|   | 7/10 [33:48<13:11, 263.94s/trial, best loss: 0.5075459554026953]\r                                                                                 \rareaUnderPR:\n\r 70%|   | 7/10 [33:48<13:11, 263.94s/trial, best loss: 0.5075459554026953]\r                                                                                 \r0.2963860528261424\n\r 70%|   | 7/10 [33:48<13:11, 263.94s/trial, best loss: 0.5075459554026953]\r                                                                                 \r-------------------\n\r 70%|   | 7/10 [34:17<13:11, 263.94s/trial, best loss: 0.5075459554026953]\r                                                                                 \rValidation Metrics:\n\r 70%|   | 7/10 [34:17<13:11, 263.94s/trial, best loss: 0.5075459554026953]\r                                                                                 \raccuracy:\n\r 70%|   | 7/10 [34:17<13:11, 263.94s/trial, best loss: 0.5075459554026953]\r                                                                                 \r0.7382290588047258\n\r 70%|   | 7/10 [34:17<13:11, 263.94s/trial, best loss: 0.5075459554026953]\r                                                                                 \rprecision:\n\r 70%|   | 7/10 [34:17<13:11, 263.94s/trial, best loss: 0.5075459554026953]\r                                                                                 \r0.8764188159369899\n\r 70%|   | 7/10 [34:17<13:11, 263.94s/trial, best loss: 0.5075459554026953]\r                                                                                 \rrecall:\n\r 70%|   | 7/10 [34:17<13:11, 263.94s/trial, best loss: 0.5075459554026953]\r                                                                                 \r0.8052371887743818\n\r 70%|   | 7/10 [34:17<13:11, 263.94s/trial, best loss: 0.5075459554026953]\r                                                                                 \rf1_score:\n\r 70%|   | 7/10 [34:17<13:11, 263.94s/trial, best loss: 0.5075459554026953]\r                                                                                 \r0.7570276923557908\n\r 70%|   | 7/10 [34:17<13:11, 263.94s/trial, best loss: 0.5075459554026953]\r                                                                                 \rf2_score:\n\r 70%|   | 7/10 [34:17<13:11, 263.94s/trial, best loss: 0.5075459554026953]\r                                                                                 \r0.33105\n\r 70%|   | 7/10 [34:17<13:11, 263.94s/trial, best loss: 0.5075459554026953]\r                                                                                 \rareaUnderROC:\n\r 70%|   | 7/10 [34:17<13:11, 263.94s/trial, best loss: 0.5075459554026953]\r                                                                                 \r0.6246518773015473\n\r 70%|   | 7/10 [34:17<13:11, 263.94s/trial, best loss: 0.5075459554026953]\r                                                                                 \rareaUnderPR:\n\r 70%|   | 7/10 [34:17<13:11, 263.94s/trial, best loss: 0.5075459554026953]\r                                                                                 \r0.2187062673557174\n\r 70%|   | 7/10 [34:17<13:11, 263.94s/trial, best loss: 0.5075459554026953]\r                                                                                 \r-------------------\n\r 70%|   | 7/10 [34:17<13:11, 263.94s/trial, best loss: 0.5075459554026953]\r                                                                                 \rModel Params:\n\r 70%|   | 7/10 [34:17<13:11, 263.94s/trial, best loss: 0.5075459554026953]\r                                                                                 \rmaxIter:\n\r 70%|   | 7/10 [34:17<13:11, 263.94s/trial, best loss: 0.5075459554026953]\r                                                                                 \r170.0\n\r 70%|   | 7/10 [34:17<13:11, 263.94s/trial, best loss: 0.5075459554026953]\r                                                                                 \rregParam:\n\r 70%|   | 7/10 [34:17<13:11, 263.94s/trial, best loss: 0.5075459554026953]\r                                                                                 \r0.13\n\r 70%|   | 7/10 [34:17<13:11, 263.94s/trial, best loss: 0.5075459554026953]\r                                                                                 \relasticNetParam:\n\r 70%|   | 7/10 [34:17<13:11, 263.94s/trial, best loss: 0.5075459554026953]\r                                                                                 \r0.7000000000000001\n\r 70%|   | 7/10 [34:17<13:11, 263.94s/trial, best loss: 0.5075459554026953]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/08/08 21:14:22 INFO mlflow.spark: Inferring pip requirements by reloading the logged model from the databricks artifact repository, which can be time-consuming. To speed up, explicitly specify the conda_env or pip_requirements when calling log_model().\n\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r                                                                                 \rmodel_logging_complete --- 296.18683552742004 seconds ---\n\r 70%|   | 7/10 [35:06<13:11, 263.94s/trial, best loss: 0.5075459554026953]\r 80%|  | 8/10 [35:06<09:08, 274.27s/trial, best loss: 0.5075459554026953]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/08/08 21:15:07 WARNING mlflow.data.spark_dataset: Failed to infer schema for Spark dataset. Exception: Unsupported Spark Type '<class 'pyspark.ml.linalg.VectorUDT'>', MLflow schema is only supported for scalar Spark types.\n\nWARNING:urllib3.connectionpool:Retrying (Retry(total=4, connect=5, read=4, redirect=5, status=5)) after connection broken by 'ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))': /api/2.0/mlflow/runs/log-metric\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r                                                                                 \r-------------------\n\r 80%|  | 8/10 [38:20<09:08, 274.27s/trial, best loss: 0.5075459554026953]\r                                                                                 \rTrain Metrics:\n\r 80%|  | 8/10 [38:20<09:08, 274.27s/trial, best loss: 0.5075459554026953]\r                                                                                 \raccuracy:\n\r 80%|  | 8/10 [38:20<09:08, 274.27s/trial, best loss: 0.5075459554026953]\r                                                                                 \r0.6517340571818171\n\r 80%|  | 8/10 [38:20<09:08, 274.27s/trial, best loss: 0.5075459554026953]\r                                                                                 \rprecision:\n\r 80%|  | 8/10 [38:20<09:08, 274.27s/trial, best loss: 0.5075459554026953]\r                                                                                 \r0.8655576640485718\n\r 80%|  | 8/10 [38:20<09:08, 274.27s/trial, best loss: 0.5075459554026953]\r                                                                                 \rrecall:\n\r 80%|  | 8/10 [38:20<09:08, 274.27s/trial, best loss: 0.5075459554026953]\r                                                                                 \r0.670735083099874\n\r 80%|  | 8/10 [38:20<09:08, 274.27s/trial, best loss: 0.5075459554026953]\r                                                                                 \rf1_score:\n\r 80%|  | 8/10 [38:20<09:08, 274.27s/trial, best loss: 0.5075459554026953]\r                                                                                 \r0.6845263574072495\n\r 80%|  | 8/10 [38:20<09:08, 274.27s/trial, best loss: 0.5075459554026953]\r                                                                                 \rf2_score:\n\r 80%|  | 8/10 [38:20<09:08, 274.27s/trial, best loss: 0.5075459554026953]\r                                                                                 \r0.48482\n\r 80%|  | 8/10 [38:20<09:08, 274.27s/trial, best loss: 0.5075459554026953]\r                                                                                 \rareaUnderROC:\n\r 80%|  | 8/10 [38:20<09:08, 274.27s/trial, best loss: 0.5075459554026953]\r                                                                                 \r0.668862245573582\n\r 80%|  | 8/10 [38:20<09:08, 274.27s/trial, best loss: 0.5075459554026953]\r                                                                                 \rareaUnderPR:\n\r 80%|  | 8/10 [38:20<09:08, 274.27s/trial, best loss: 0.5075459554026953]\r                                                                                 \r0.32511506716397404\n\r 80%|  | 8/10 [38:20<09:08, 274.27s/trial, best loss: 0.5075459554026953]\r                                                                                 \r-------------------\n\r 80%|  | 8/10 [38:49<09:08, 274.27s/trial, best loss: 0.5075459554026953]\r                                                                                 \rValidation Metrics:\n\r 80%|  | 8/10 [38:49<09:08, 274.27s/trial, best loss: 0.5075459554026953]\r                                                                                 \raccuracy:\n\r 80%|  | 8/10 [38:49<09:08, 274.27s/trial, best loss: 0.5075459554026953]\r                                                                                 \r0.5378860809105003\n\r 80%|  | 8/10 [38:49<09:08, 274.27s/trial, best loss: 0.5075459554026953]\r                                                                                 \rprecision:\n\r 80%|  | 8/10 [38:49<09:08, 274.27s/trial, best loss: 0.5075459554026953]\r                                                                                 \r0.9066793808488934\n\r 80%|  | 8/10 [38:49<09:08, 274.27s/trial, best loss: 0.5075459554026953]\r                                                                                 \rrecall:\n\r 80%|  | 8/10 [38:49<09:08, 274.27s/trial, best loss: 0.5075459554026953]\r                                                                                 \r0.5080230913939081\n\r 80%|  | 8/10 [38:49<09:08, 274.27s/trial, best loss: 0.5075459554026953]\r                                                                                 \rf1_score:\n\r 80%|  | 8/10 [38:49<09:08, 274.27s/trial, best loss: 0.5075459554026953]\r                                                                                 \r0.6005275131053028\n\r 80%|  | 8/10 [38:49<09:08, 274.27s/trial, best loss: 0.5075459554026953]\r                                                                                 \rf2_score:\n\r 80%|  | 8/10 [38:49<09:08, 274.27s/trial, best loss: 0.5075459554026953]\r                                                                                 \r0.47226\n\r 80%|  | 8/10 [38:49<09:08, 274.27s/trial, best loss: 0.5075459554026953]\r                                                                                 \rareaUnderROC:\n\r 80%|  | 8/10 [38:49<09:08, 274.27s/trial, best loss: 0.5075459554026953]\r                                                                                 \r0.6461485471666844\n\r 80%|  | 8/10 [38:49<09:08, 274.27s/trial, best loss: 0.5075459554026953]\r                                                                                 \rareaUnderPR:\n\r 80%|  | 8/10 [38:49<09:08, 274.27s/trial, best loss: 0.5075459554026953]\r                                                                                 \r0.23548234919141614\n\r 80%|  | 8/10 [38:49<09:08, 274.27s/trial, best loss: 0.5075459554026953]\r                                                                                 \r-------------------\n\r 80%|  | 8/10 [38:49<09:08, 274.27s/trial, best loss: 0.5075459554026953]\r                                                                                 \rModel Params:\n\r 80%|  | 8/10 [38:49<09:08, 274.27s/trial, best loss: 0.5075459554026953]\r                                                                                 \rmaxIter:\n\r 80%|  | 8/10 [38:49<09:08, 274.27s/trial, best loss: 0.5075459554026953]\r                                                                                 \r160.0\n\r 80%|  | 8/10 [38:49<09:08, 274.27s/trial, best loss: 0.5075459554026953]\r                                                                                 \rregParam:\n\r 80%|  | 8/10 [38:49<09:08, 274.27s/trial, best loss: 0.5075459554026953]\r                                                                                 \r0.39\n\r 80%|  | 8/10 [38:49<09:08, 274.27s/trial, best loss: 0.5075459554026953]\r                                                                                 \relasticNetParam:\n\r 80%|  | 8/10 [38:49<09:08, 274.27s/trial, best loss: 0.5075459554026953]\r                                                                                 \r0.1\n\r 80%|  | 8/10 [38:49<09:08, 274.27s/trial, best loss: 0.5075459554026953]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/08/08 21:18:55 INFO mlflow.spark: Inferring pip requirements by reloading the logged model from the databricks artifact repository, which can be time-consuming. To speed up, explicitly specify the conda_env or pip_requirements when calling log_model().\n\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r                                                                                 \rmodel_logging_complete --- 274.498642206192 seconds ---\n\r 80%|  | 8/10 [39:41<09:08, 274.27s/trial, best loss: 0.5075459554026953]\r 90%| | 9/10 [39:41<04:34, 274.41s/trial, best loss: 0.5075459554026953]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/08/08 21:19:42 WARNING mlflow.data.spark_dataset: Failed to infer schema for Spark dataset. Exception: Unsupported Spark Type '<class 'pyspark.ml.linalg.VectorUDT'>', MLflow schema is only supported for scalar Spark types.\n\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r                                                                                 \r-------------------\n\r 90%| | 9/10 [43:05<04:34, 274.41s/trial, best loss: 0.5075459554026953]\r                                                                                 \rTrain Metrics:\n\r 90%| | 9/10 [43:05<04:34, 274.41s/trial, best loss: 0.5075459554026953]\r                                                                                 \raccuracy:\n\r 90%| | 9/10 [43:05<04:34, 274.41s/trial, best loss: 0.5075459554026953]\r                                                                                 \r0.730173986652554\n\r 90%| | 9/10 [43:05<04:34, 274.41s/trial, best loss: 0.5075459554026953]\r                                                                                 \rprecision:\n\r 90%| | 9/10 [43:05<04:34, 274.41s/trial, best loss: 0.5075459554026953]\r                                                                                 \r0.8451982242636832\n\r 90%| | 9/10 [43:05<04:34, 274.41s/trial, best loss: 0.5075459554026953]\r                                                                                 \rrecall:\n\r 90%| | 9/10 [43:05<04:34, 274.41s/trial, best loss: 0.5075459554026953]\r                                                                                 \r0.8131017967497374\n\r 90%| | 9/10 [43:05<04:34, 274.41s/trial, best loss: 0.5075459554026953]\r                                                                                 \rf1_score:\n\r 90%| | 9/10 [43:05<04:34, 274.41s/trial, best loss: 0.5075459554026953]\r                                                                                 \r0.7372817527113507\n\r 90%| | 9/10 [43:05<04:34, 274.41s/trial, best loss: 0.5075459554026953]\r                                                                                 \rf2_score:\n\r 90%| | 9/10 [43:05<04:34, 274.41s/trial, best loss: 0.5075459554026953]\r                                                                                 \r0.37934\n\r 90%| | 9/10 [43:05<04:34, 274.41s/trial, best loss: 0.5075459554026953]\r                                                                                 \rareaUnderROC:\n\r 90%| | 9/10 [43:05<04:34, 274.41s/trial, best loss: 0.5075459554026953]\r                                                                                 \r0.6432970619979722\n\r 90%| | 9/10 [43:05<04:34, 274.41s/trial, best loss: 0.5075459554026953]\r                                                                                 \rareaUnderPR:\n\r 90%| | 9/10 [43:05<04:34, 274.41s/trial, best loss: 0.5075459554026953]\r                                                                                 \r0.2963860528261424\n\r 90%| | 9/10 [43:05<04:34, 274.41s/trial, best loss: 0.5075459554026953]\r                                                                                 \r-------------------\n\r 90%| | 9/10 [43:31<04:34, 274.41s/trial, best loss: 0.5075459554026953]\r                                                                                 \rValidation Metrics:\n\r 90%| | 9/10 [43:31<04:34, 274.41s/trial, best loss: 0.5075459554026953]\r                                                                                 \raccuracy:\n\r 90%| | 9/10 [43:31<04:34, 274.41s/trial, best loss: 0.5075459554026953]\r                                                                                 \r0.7382290588047258\n\r 90%| | 9/10 [43:31<04:34, 274.41s/trial, best loss: 0.5075459554026953]\r                                                                                 \rprecision:\n\r 90%| | 9/10 [43:31<04:34, 274.41s/trial, best loss: 0.5075459554026953]\r                                                                                 \r0.8764188159369899\n\r 90%| | 9/10 [43:31<04:34, 274.41s/trial, best loss: 0.5075459554026953]\r                                                                                 \rrecall:\n\r 90%| | 9/10 [43:31<04:34, 274.41s/trial, best loss: 0.5075459554026953]\r                                                                                 \r0.8052371887743818\n\r 90%| | 9/10 [43:31<04:34, 274.41s/trial, best loss: 0.5075459554026953]\r                                                                                 \rf1_score:\n\r 90%| | 9/10 [43:31<04:34, 274.41s/trial, best loss: 0.5075459554026953]\r                                                                                 \r0.7570276923557908\n\r 90%| | 9/10 [43:31<04:34, 274.41s/trial, best loss: 0.5075459554026953]\r                                                                                 \rf2_score:\n\r 90%| | 9/10 [43:31<04:34, 274.41s/trial, best loss: 0.5075459554026953]\r                                                                                 \r0.33105\n\r 90%| | 9/10 [43:31<04:34, 274.41s/trial, best loss: 0.5075459554026953]\r                                                                                 \rareaUnderROC:\n\r 90%| | 9/10 [43:31<04:34, 274.41s/trial, best loss: 0.5075459554026953]\r                                                                                 \r0.6246518773015473\n\r 90%| | 9/10 [43:31<04:34, 274.41s/trial, best loss: 0.5075459554026953]\r                                                                                 \rareaUnderPR:\n\r 90%| | 9/10 [43:31<04:34, 274.41s/trial, best loss: 0.5075459554026953]\r                                                                                 \r0.2187062673557174\n\r 90%| | 9/10 [43:31<04:34, 274.41s/trial, best loss: 0.5075459554026953]\r                                                                                 \r-------------------\n\r 90%| | 9/10 [43:31<04:34, 274.41s/trial, best loss: 0.5075459554026953]\r                                                                                 \rModel Params:\n\r 90%| | 9/10 [43:31<04:34, 274.41s/trial, best loss: 0.5075459554026953]\r                                                                                 \rmaxIter:\n\r 90%| | 9/10 [43:31<04:34, 274.41s/trial, best loss: 0.5075459554026953]\r                                                                                 \r100.0\n\r 90%| | 9/10 [43:31<04:34, 274.41s/trial, best loss: 0.5075459554026953]\r                                                                                 \rregParam:\n\r 90%| | 9/10 [43:31<04:34, 274.41s/trial, best loss: 0.5075459554026953]\r                                                                                 \r0.23\n\r 90%| | 9/10 [43:31<04:34, 274.41s/trial, best loss: 0.5075459554026953]\r                                                                                 \relasticNetParam:\n\r 90%| | 9/10 [43:31<04:34, 274.41s/trial, best loss: 0.5075459554026953]\r                                                                                 \r0.4\n\r 90%| | 9/10 [43:31<04:34, 274.41s/trial, best loss: 0.5075459554026953]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/08/08 21:23:36 INFO mlflow.spark: Inferring pip requirements by reloading the logged model from the databricks artifact repository, which can be time-consuming. To speed up, explicitly specify the conda_env or pip_requirements when calling log_model().\n\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r                                                                                 \rmodel_logging_complete --- 280.3648474216461 seconds ---\n\r 90%| | 9/10 [44:21<04:34, 274.41s/trial, best loss: 0.5075459554026953]\r100%|| 10/10 [44:22<00:00, 276.30s/trial, best loss: 0.5075459554026953]\r100%|| 10/10 [44:22<00:00, 266.21s/trial, best loss: 0.5075459554026953]\n"
     ]
    }
   ],
   "source": [
    "from hyperopt import fmin, tpe, Trials\n",
    "import mlflow\n",
    "\n",
    "start_time = time.time()\n",
    "mlflow.pyspark.ml.autolog(log_models=False)\n",
    "num_evals = 10\n",
    "trials = Trials()\n",
    "best_hyperparam = fmin(fn=objective_function,\n",
    "                       space = search_space,\n",
    "                       algo=tpe.suggest,\n",
    "                       max_evals = num_evals,\n",
    "                       trials=trials,\n",
    "                       rstate=np.random.default_rng(42))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6fd2b284-bfdc-4da4-87a9-5ddfea03e88e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "3734871"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_12m.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2682166-0e2c-46bc-bd63-4f638a5fb80b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "9192041"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_12m.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8968a29c-4666-4e7d-972b-3bd2cb82e54a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "12926912"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_12m.count() + df_train_12m.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "569b9372-4cec-4523-9a5c-e7b9f598bc80",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [
    {
     "elements": [],
     "globalVars": {},
     "guid": "c3d884b2-12b2-43af-aa2f-90bdb2c52111",
     "layoutOption": {
      "grid": true,
      "stack": true
     },
     "nuid": "d404ab4d-79db-4b22-8327-4df01cb5dc02",
     "origId": 3984215836277776,
     "title": "Untitled",
     "version": "DashboardViewV1",
     "width": 1024
    },
    {
     "elements": [],
     "globalVars": {},
     "guid": "196a2430-56db-4aa7-b86f-72c7a41c12c0",
     "layoutOption": {
      "grid": true,
      "stack": true
     },
     "nuid": "e5336f6e-92ad-4e64-8bc1-efa53b3cc9cb",
     "origId": 3984215836277777,
     "title": "Summary Table",
     "version": "DashboardViewV1",
     "width": 1024
    }
   ],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "FP_Section2_Group4_LogReg_12months",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
